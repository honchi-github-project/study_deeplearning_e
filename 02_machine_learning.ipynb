{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4fhror4LEgr"
   },
   "source": [
    "iStudy ACADEMY 現場で潰しが効くディープラーニング講座 視聴課題レポート 機械学習（講義動画と実装演習）\n",
    "\n",
    "# 機械学習\n",
    "\n",
    "## 参考文献・URL\n",
    "\n",
    "- [統計WEB - 統計学の時間](https://bellcurve.jp/statistics/course/)\n",
    "- [AI人工知能テクノロジー](https://newtechnologylifestyle.net/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "druxFGqNOdy4"
   },
   "source": [
    "# 本編前\n",
    "\n",
    "## 機械学習の注意点\n",
    "- 課題解決にあたって必ずしも機械学習を使う必要はなく、ルールベースで解決できるのならそれで構わない\n",
    "  - クラウディア社データサイエンティスト\n",
    "- 機械学習のデメリットも考えること\n",
    "  - 技術的な前提レベルが高い\n",
    "  - テストがしにくい（Aを投入し必ずBが返ってくるわけではない）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erWaiWY6VPQ9"
   },
   "source": [
    "# 第１章：線形回帰モデル\n",
    "\n",
    "## 本編\n",
    "\n",
    "- 機械学習の定義\n",
    "  - コンピュータ・プログラムを経験によって自動的に改善していく\n",
    "  - 1997 トム・ミッチェル\n",
    "    - タスクTを性能指標Pで測定  \n",
    "    →経験E(データ)により改善  \n",
    "    →タスクT及び性能指標Pが経験Eから学習\n",
    "  - データを与えるほど良くなる\n",
    "  - 線形回帰モデルやロジスティック回帰モデルなど\n",
    "    - 例：過去の株価の遷移をデータとして投入し、前日終値を元に本日の株価予想を行う  \n",
    "実際値と計算値の差分が性能指標Pとして計測可能\n",
    "\n",
    "### 考察\n",
    "\n",
    "- データを与えるほど良くなるとのことだが、DL(特に教師あり学習)の場合は過学習が発生するため線形回帰やロジスティック回帰モデルに限った話になると思う\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_dpk3Nx7xZ9"
   },
   "source": [
    "## 線形回帰\n",
    "\n",
    "- 線形回帰モデルは回帰問題を解くアルゴリズムであり、教師あり学習\n",
    "- 線形回帰はデータを直線で、非線形回帰は曲線で近似\n",
    "- 説明変数が一次元は単回帰、多次元は重回帰\n",
    "- ある入力(数値)から出力(連続値)を予想\n",
    "  - 入力はm次元ベクトルであり各要素は説明変数(特徴量)\n",
    "  - 出力はスカラーで目的変数\n",
    "  - 入力とm次元パラメータの `線形結合` を出力するモデル\n",
    "    - $w = (w_1,w_2, ... ,w_m)^T \\in \\mathbb{R^m}$\n",
    "    - $\\hat{y} = w^T x + b = \\Sigma_{j=1}^m w_j x_j + b$　　(線形結合)\n",
    "    - $y = w_0 + w_1 x_1 + \\epsilon$　　　　　　　(線形単回帰モデル)\n",
    "    - $y = w_0 + w_1 x_1 + w_2 x_2 + ... + \\epsilon$　　 (線形重回帰モデル)\n",
    "  - $ \\hat{y} $ の頭についているのはハットといい、統計学では数式を計算し算出した予想値の意味\n",
    "  - $ w $ は説明変数(特徴量) へ掛けるモデルのパラメータであり、個々に乗算し全加算するので説明変数(特徴量)への重みである\n",
    "  - $ w $ は未知パラメータであり最小二乗法で推定\n",
    "- 回帰係数の推定には２パターンある\n",
    "  - 最小二乗法：回帰係数が正規分布にしたがう仮定は不要\n",
    "  - 最尤法：回帰係数が正規分布に従う仮定が必要\n",
    "    - 正規分布は平均値 $ u $ や 分散 $ \\sigma^2 $ のパラメータもあり、仮定することで詳細な分析が可能となる\n",
    "  - **回帰係数の推定に関してはどちらを使っても一致する**\n",
    "\n",
    "### 参考URL\n",
    "\n",
    "- [統計WEB 27-1.単回帰分析](https://bellcurve.jp/statistics/course/9700.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azuRkv3K70qc"
   },
   "source": [
    "## データ分割/学習\n",
    "\n",
    "- 学習用データを更に復数のモデルに分割投入し学習させる\n",
    "- 復数のモデルに検証用データを投入し結果を検証する\n",
    "-  データとモデル出力の差を見てモデルを決めていく必要がある。\n",
    "  - 平均二乗誤差: その誤差を求める手法\n",
    "  - 最小二乗法 : 平均二乗誤差を最小とするパラメータを探索。勾配が0になる点を求めれば良い\n",
    "\n",
    "### 考察・調査\n",
    "\n",
    "- なぜ分割する必要がある？\n",
    "  - すべて学習用データとして使用した場合、過度に適合したモデルが出来上がってしまい、逆に精度が低くなる　→　**過学習**\n",
    "  - 検証用データを残すことで学習の検証を行うことが可能　→　**汎化性能の確認**\n",
    "- 分割手法は？\n",
    "  - Hold-out法 : データ全体を学習用と検証用に分割 (例: 6対4)\n",
    "  - Cross Validation法 : 別名K-分割交差検証。データ全体をk個に分割、１つを検証用とし残るk-1個を学習用にする。k回検証される\n",
    "  - Leave One Out法 : データ全体の内１つだけを検証用にする。その後学習用と検証用データを入れ替え繰り返し、全てのケースがテスト事例となるよう検証を繰り返す\n",
    "- データ個数で分割手法は変わるか？\n",
    "  - データサンプル10個以下 : 手法に関係なく効果は期待できない\n",
    "  - データサンプル100個以下 : Leave One Out法が有効だが精度はあまり期待できない\n",
    "  - データサンプル1,000個以下 : Cross Validationで10分割が丁度良い。\n",
    "  - データサンプル10,000個以下 : Cross Validationで10分割以内か、Hold-out法が良い\n",
    "  - データサンプル数100,000個以下 : Hold-out法は無理。高性能なマシンスペックが必要(研究機関とか)\n",
    "\n",
    "#### 参考URL\n",
    "\n",
    "- [機械学習、ディープラーニングでの学習データとテストデータの分割手法について](https://newtechnologylifestyle.net/機械学習、ディープラーニングでの学習データと/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4I9H8JWZ72Xb"
   },
   "source": [
    "## ハンズオンの結果と考察\n",
    "\n",
    "### 実施結果\n",
    "\n",
    "- [線形回帰モデル-Boston Hausing Data-](./02_exercise/skl_regression.ipynb)\n",
    "  - 変更箇所は【レポート提出者変更】とコメントで囲って記載\n",
    "\n",
    "### 考察\n",
    "\n",
    "- 線形回帰モデルでは誤差逆伝搬を用いたパラメータ更新ではなく、  \n",
    "最小二乗法を元にした統計学的な解析によりパラメータ算出を行うことが理解できた。\n",
    "- 講義動画にて「最尤法は回帰係数が正規分布に従う仮定が必要」との補足があったが、  \n",
    "[最尤推定・最尤法](https://ja.wikipedia.org/wiki/%E6%9C%80%E5%B0%A4%E6%8E%A8%E5%AE%9A) に 「統計学において、与えられたデータからそれが従う確率分布の母数を点推定する方法」とある。  \n",
    "確率分布のため正規分布を仮定したいことが理解できた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c8XKJrfWVZvl"
   },
   "source": [
    "# 第２章：非線形回帰モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30IzYZCSp91u"
   },
   "source": [
    "## 非線形回帰モデル\n",
    "\n",
    "- 現実問題として、データが綺麗に線形に並んでいることは少ない\n",
    "- 非線形のデータを捉える、非線形回帰モデルを考える必要がある\n",
    "- 非線形回帰モデルも線形回帰モデルと同様に考えることができる\n",
    "  - 回帰関数として非線形関数とパラメータベクトルの線形結合を使用\n",
    "  - 基底関数と呼ばれ、未知パラメータを線形回帰モデルと同様に最小二乗法や最尤法で推定可能\n",
    "  - 基底関数は以下がよく使われる\n",
    "    - 多項式関数\n",
    "    - ガウス型基底関数\n",
    "    - スプライン関数 / Bスプライン関数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mmWM8P53qCLF"
   },
   "source": [
    "## 正則化\n",
    "\n",
    "- 非線形の場合、過学習や未学習が起こる\n",
    "  - 　未学習とは　学習データに対して誤差が大きい\n",
    "  - 　過学習とは　学習データに対して誤差が小さすぎ、汎化性能が低い\n",
    "  - 汎化性能とは　学習データ以外に未知のデータに対する予測性能\n",
    "\n",
    "=> ***講義終盤に講師より解説：非線形回帰のみではなく線形回帰の場合も過学習は起こり得る。勘違いしやすいので注意。***\n",
    "\n",
    "### 正則化法\n",
    "  - 過学習を回避する手法\n",
    "  - 正則化しすぎると学習が妨げられ未学習となる\n",
    "  - モデルの複雑さに伴い値が大きくなるペナルティ項(正則化項)を導入\n",
    "    - その関数の最小化を考える\n",
    "\n",
    "#### 考察\n",
    "\n",
    "- 正則化法は回帰曲線をデータに近づけさせすぎないようにする手法と捉えられる\n",
    "- 過学習を回避する方法として正則化の話が出るということは、それだけ過学習を避けたい、機械学習にとって未学習より過学習が恐ろしいと言える\n",
    "\n",
    "### ペナルティ項目\n",
    "\n",
    "- ペナルティ項目で利用する関数(ない場合は最小二乗推定量)\n",
    "\n",
    "| 利用するノルム | 推定量 | 推定手法 | 意味 |\n",
    "| - | - | - | - |\n",
    "| L2ノルム | Ridge推定量 | 縮小推定 |パラメータを０に近づけるよう推定 |\n",
    "| L1ノルム | Lasso推定量 | スパース推定 | いくつかのパラメータを０に推定 |\n",
    "\n",
    "### モデルの選択\n",
    "\n",
    "- 正則化パラメータはクロスバリデーション(交差検証)で選択する\n",
    "- データ分割手法としてホールドアウト法は出てきたが、学習用とテスト用とのバランスを取らなければならない問題がある\n",
    "- クロスバリデーションは分割位置を変えた復数パターンのデータセットを用意する\n",
    "  - 復数のモデルへ適用、CV値(精度)の平均を取る事ができる。\n",
    "  - 一番CV値が低いモデルが最善として選択する事ができる。\n",
    "\n",
    "| 名称 | 手法 | 特記 |\n",
    "| - | - | - |\n",
    "| Hold-out法 | データ全体を学習用と検証用に分割 (例: 6対4) | 大量データが必要 |\n",
    "| Cross Validation法 | データ全体をk個に分割、１つを検証用とし残るk-1個を学習用にする。k回検証される | Hold-out法の欠点を補う |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kz8x4TleqQ78"
   },
   "source": [
    "## ハンズオンの結果と考察\n",
    "\n",
    "- 講義動画では「ハンズオンが終わりまして」とのみ述べられている\n",
    "- 講義PDFには https://github.com/studyaigit/StudyAI-M-L/tree/master/skl_MultivariateAnalysis のみ記載されており 404 Not Foundエラー\n",
    "- ZIPで頂いた内容にも非線形回帰モデルは含まれておらず、実施不可能。\n",
    "\n",
    "=> ***ハンズオン対象外と思われる。スキップ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjGuhvuAVhMI"
   },
   "source": [
    "# 第３章：ロジスティック回帰モデル\n",
    "\n",
    "- 回帰が名前に入っているが、 **回帰ではない**\n",
    "- ロジスティック回帰モデルは分類問題(クラス分類)のモデル\n",
    "    - 入力値からラベルを予測するモデル\n",
    "    - 入力 : パラメータの線形結合をシグモイド関数（生起関数）に入力\n",
    "    - 出力 : 0〜1の確率変数\n",
    "    - 特徴 : あるものに分類して良い確率を求める。例：0.5以上は1、0.5以下は0\n",
    "- 線形回帰との類似性\n",
    "    - 入力とパラメータの線形結合を行う点は線形回帰と同様\n",
    "    - 線形結合の結果をロジスティック関数の入力とする\n",
    "    - $ \\hat{y} = w^T x + b = \\sigma_{j=1}^{m}w_j x_j + b $\n",
    "- シグモイド関数\n",
    "    - 入力 : 実数\n",
    "    - 出力 : 0〜1の確率変数\n",
    "    - 特徴 : パラメータが変わるとシグモイド関数の結果が変わる\n",
    "        - ゲインa増加→x=0付近の曲線勾配が増加\n",
    "        - ゲインaを極大化するとステップ関数に近づく\n",
    "        - バイアス変化は段差の位置\n",
    "    - $ \\sigma(x) = \\frac{1}{1+\\exp(-ax)} $\n",
    "    - $ \\sigma'(x) = (1-\\sigma(x))\\sigma(x) $\n",
    "    - 性質\n",
    "        - シグモイド関数の微分は、シグモイド関数自身で表現可能 **よって計算が簡単**\n",
    "        - 尤度関数の微分を行う際にも活用される\n",
    "- 機械学習の鉄板的考え方\n",
    "    - MSEを作り平均二乗誤差を計算しそれを最小化するパラメータを求める\n",
    "    - 何かの基準を作り、その基準を最小化するパラメータを求める\n",
    "    - その基準を目的関数といい、その目的関数を最小化するパラメータを見つけるのが **機械学習の王道**\n",
    "- ロジスティック回帰の考え方\n",
    "    - 誘導関数を作り、誘導関数を最大化するパラメータを求める\n",
    "    - 何かを最大化・最小化するものを求めるのを **最適化問題** という\n",
    "    - $Y=1$ になる確率とシグモイド関数を対応させる\n",
    "        - $ P(Y=1 | x) = \\sigma(w_0 + w_1 x_1 + ... + w_m x_m) $\n",
    "        - 求めたい値 未知のデータが与えられた際に $Y=1$ になる確率 = $\\sigma($ データのパラメータに対する線形結合 $)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最尤推定\n",
    "\n",
    "- ロジスティック回帰モデルの結果として分類するための幾何学的なグラフを描くことができるが、そのグラフを描くパラメータを求めるのが最尤推定\n",
    "- 確率 : **[演算方向 パラメータ→特定データ]** あるパラメータ分布から特定データがどれだけ得られやすいか\n",
    "- 尤度 : **[演算方向 特定データ→パラメータ]** あるデータを得たとき分布のパラメータが特定の値であることがどれだけありえるのか？\n",
    "- 補足 : 確率はパラメータを固定しデータが変化、尤度はデータを固定してパラメータが変化  \n",
    "\n",
    "### 尤度関数とは\n",
    "\n",
    "- 前提条件に従って結果が出る場合、その前提条件が「何々だったのか」を変数とする関数\n",
    "- モデルの出力Yの確率\n",
    "\n",
    "| 確率 | 数式 | 備考 |\n",
    "| --- | --- | --- |\n",
    "| Y=1 | $ P(Y=1|x) = p $ | - |\n",
    "| Y=0 | $ P(Y=0|x) = 1-P(Y=1|x) = 1-p $ | - |\n",
    "| Y=t | $ P(Y=t|x) = P(Y=1|x)^t P(Y=0|x)^{1-t} = P^t(1-p)^{1-t} $ | 確率変数Yはベールヌイ試行に従う |\n",
    "\n",
    "- ベールヌイ試行とは : AかBのどちらかしか起こらないといった事象を起こさせること\n",
    "     - Y=t の式の形であれば t=0/1 のどちらの場合も表現可能\n",
    "- 尤度関数を最大化するよりも、対数尤度関数を最大化するほうが楽\n",
    "    - 尤度関数を最大化するパラメータと対数尤度関数を最大化するパラメータは同じ\n",
    "    - 対数尤度関数は 掛け算→足し算 指数→積 で行える\n",
    "    - 対数尤度関数に **マイナス演算し最小化** で統一するのが良い\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配降下法\n",
    "\n",
    "- 線形回帰モデル(最小二乗法)の場合、MSEのパラメータを微分し０になる値を解析的に求めることができた\n",
    "- ロジスティック回帰モデル(最尤法)の場合、同様の方法で\"数学的に陽に解けない\"ため、反復計算を行い随時パラメータを更新する\n",
    "    - 学習率 $\\eta$ (イータ)と呼ばれるハイパーパラメータを有し、パラメータの収束しやすさを調整可能\n",
    "    - $w^{(k+1)} = w^{(k)} - \\eta \\frac{\\partial E(w,b)}{\\partial w}$  \n",
    "$b^{(b+1)} = b^{(k)} - \\eta \\frac{\\partial E(w,b)}{\\partial b}$\n",
    "- 反復計算を行い更新されなくなると、勾配＝０ということ\n",
    "\n",
    "### 短所\n",
    "\n",
    "- データ個数Nが巨大な場合 **メモリに乗り切らない, 計算量膨大** の問題が出る。\n",
    "    - 理由 : １つのパラメータ更新に際し全データで計算し和を求めるため\n",
    "    - 　例 : Nが1万〜100万以上など\n",
    "- 回避方法はあり、確率的勾配法で解決可能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確率的勾配法\n",
    "\n",
    "### 勾配降下法のデモ動画\n",
    "\n",
    "- 初期値を与え、逐次的にパラメータを更新し（尤度の高い方へ近づく)、収束した時点の値を採用する\n",
    "- 学習率 $\\eta$ (イータ)について\n",
    "    - $\\eta$ が大きい : 更新の歩幅が大きくなり、尤度の高い方へ早く近づくが、収束しなくなる\n",
    "    - $\\eta$ が小さい : 収束までに時間がかかる\n",
    "\n",
    "### 疑問\n",
    "\n",
    "- 学習率 $\\eta$ ハイパーパラメータだが、最適値を簡易に求める手段はないか\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの評価\n",
    "\n",
    "### 混同行列(Confusion Matrix)\n",
    "\n",
    "| -          | 正解が陽 | 正解が陰 |\n",
    "|:----------:|:-------:|:-------:|\n",
    "| 分類結果が陽 | TP<br>真陽性<br>True Positive | FP<br>偽陽性<br>False Positive |\n",
    "| 分類結果が陰 | FN<br>偽陰性<br>False Negative | TN<br>真陰性<br>True Negative |\n",
    "\n",
    "- 混同行列は、ある手法を用いて分類処理を行った場合に、分類した値(陰/陽)とその正誤(真/偽)について結果をまとめた表\n",
    "\n",
    "### 分類の評価方法\n",
    "\n",
    "- 分類結果から得られた混同行列をベースに、分類の指標値が計算可能\n",
    "\n",
    "| 名前 | 算出式 | 概要 |\n",
    "| - | - | - |\n",
    "| 正解率(accuracy)                | $\\frac{TP+TN}{TP+FP+TN+FN}$ | 分類の正解率。全体のうちでどれだけが正しく分類できたか   |\n",
    "| 再現率(Recall)/感度(sensitivity) | $\\frac{TP}{TP+FN}$          | 陽性のデータのうち、どれだけを陽性と分類できたか        |\n",
    "| 適合率(precision)               | $\\frac{TP}{TP+FP}$          | 陽性と判断されたデータのうち、どれだけが正しく分類できたか |\n",
    "| 特異性(specificity)             | $\\frac{TN}{TN+FP}$          | 陰性のデータのうち、どれだけを陰性と分類できたか        |\n",
    "| F値(F-measure)                 | $\\frac{2 Recall \\cdot Precision}{Recall+Precision}$ | 適合率(精度)と再現率の調和平均<br>適合率と再現率は相反の関係 |\n",
    "\n",
    "- 実際的な例\n",
    "    - 適合率(Precision)\n",
    "        - 見逃しが多くてもより正確な予測をしたい場合\n",
    "        - 重要なメールはスパム判定されたくないが、スパムのすり抜けは許容\n",
    "            - ***正解が陽=スパムとした場合, FNを許容***\n",
    "    - 再現率(Recall)\n",
    "        - 誤りが多少多くても抜け漏れを少なくしたい場合\n",
    "        - 珍しい病気の検診で病気であると誤判定するケースが多少あっても、再検査すれば良い\n",
    "            - ***正解が陽=病気とした場合, FPを許容***\n",
    "\n",
    "### 参考URL\n",
    "\n",
    "- [混同しやすい混同行列](http://www.baru-san.net/archives/141)\n",
    "- [混同行列](https://dev.classmethod.jp/cloud/aws/aml-evaluation-measures/)\n",
    "- [F値](http://ibisforest.org/index.php?F%E5%80%A4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ハンズオンの結果と考察\n",
    "\n",
    "### 実施結果\n",
    "\n",
    "- [ロジスティック回帰モデル-タイタニック号の生存予測-](./02_exercise/skl_logistic_regression.ipynb)\n",
    "  - 変更箇所は【レポート提出者変更】とコメントで囲って記載\n",
    "\n",
    "### 考察\n",
    "\n",
    "- タイタニック号で死亡者が多数出た要因の一つとして、救命ボートが68艇搭載可能なところ20艇のみの搭載だったことも関係しているのではないか。乗客が利用する客室と救命ボートとの位置関係を割り出せれば、距離(遠)=死亡、距離(近)=生存の傾向を求められる可能性を考えた。\n",
    "- タイタニックデータのように、一部欠損値を補完した場合、そのデータの検定が必要なのではないかと考えた。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTsHDFYvHG-9"
   },
   "source": [
    "# 第４章：主成分分析\n",
    "\n",
    "## 主成分分析\n",
    "\n",
    "- 多次元データの持つ情報をできるだけ損なわず低次元空間に情報を縮約する、次元の縮小に関する手法\n",
    "- 多次元データを２次元や３次元に縮約することで、視覚化が可能\n",
    "- 射影したデータのばらつきが大きいほど、元の情報を多く含んでいる\n",
    "    - 射影したデータの分散が最大化する射影軸(線形変換)を求める\n",
    "    - 分散共分散を用いる\n",
    "    - 線形変換後の座標 $ \\displaystyle s_j = (s_{1j}, ... ,s_{nj})^T = \\bar{X}a_j $\n",
    "    - 線形変換後の分散 $ \\displaystyle Var(s_j) = \\frac{1}{n} s_j^T s_j = \\frac{1}{n}(\\bar{X}a_j)^T(\\bar{X}a_j) = \\frac{1}{n}a_j^T \\bar{X}\\bar{X}a_j = a_j^T Var(\\bar{X})a_j $\n",
    "    - 射影後の軸上の分散 $ \\displaystyle Var(s_j) $ は元の軸上の分散共分散 $ \\displaystyle Var(\\bar{X}) $ で求められる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 固有値との関係\n",
    "\n",
    "- 係数ベクトルは線形変換後の値を変化させる\n",
    "- 係数ベクトル $a_j$ を解く最適化問題とおき、ラグランジュ関数を最大にする係数ベクトルを求める\n",
    "    - 分散を最大化する係数ベクトル = 分散共分散行列の固有ベクトル\n",
    "    - $ Var(\\bar{X})a_j = \\lambda a_j $\n",
    "- 射影ベクトルの向きは固有ベクトル\n",
    "- 主成分の分散は主成分が持つ情報量\n",
    "    - = 分散共分散の固有値\n",
    "- 分散共分散行列は実対称行列、よって固有ベクトルはすべて直行\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主成分と寄与率\n",
    "\n",
    "- k番目の固有値に対応する固有ベクトルで変換された特徴量を第k主成分と呼ぶ\n",
    "    - 第１主成分：射影したデータの分散が最大となるような軸\n",
    "    - 第２主成分：第１主成分と直交する軸の中で、軸上に射影したデータの分散が最大となる軸\n",
    "- 第k主成分の全分散に対する割合を第k成分の寄与率と呼ぶ（情報量の割合）\n",
    "    - 寄与率を累積したものを累積寄与率と呼び、情報の損失程度を見ることができる\n",
    "- 主成分分析は次元圧縮により寄与率の低いデータを無視するということ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 気づき・調査\n",
    "\n",
    "- G検定ではニューラルネットワークにて次元削減し特徴を学習させる「オートエンコーダ」があったが別もの\n",
    "- 主成分分析では、分散共分散行列から分析を行う以外にも、相関行列から行う方法もあり、結果が異なる\n",
    "- 個々の入力データが異なる単位の場合、基準化して分析を行う必要がある\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考URL\n",
    "\n",
    "- [意味がわかる主成分分析](https://qiita.com/NoriakiOshita/items/460247bb57c22973a5f0)\n",
    "- [主成分分析 - 統計科学研究所](https://statistics.co.jp/reference/software_R/statR_9_principal.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ハンズオン\n",
    "\n",
    "- 結果のみ(乳がん検査データとMNIST)\n",
    "    - 主成分分析によりプロット数が減りデータが見やすくなったと考えられる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7AN3lwPVHuAL"
   },
   "source": [
    "# 第５章：アルゴリズム\n",
    "\n",
    "## k近傍法(kNN - k-nearest neighbor algorithm)\n",
    "\n",
    "- 分類問題のための機械学習手法\n",
    "- 近傍k個のデータをピックアップし、その中で最も多いラベルを、その分類として選択する\n",
    "- kを変えると結果も変わる（それぞれに分類されるラベルが変わる可能性がある）\n",
    "- kを大きくすると決定境界は滑らかになる\n",
    "\n",
    "### 調査\n",
    "\n",
    "- kは正の整数で、一般的に小さい\n",
    "- 二項分類の場合、kを奇数にすると同ラベル数で分類できなくなる問題を回避可能\n",
    "- データが増えるに従い計算量も膨大となる。\n",
    "    - 様々な最近傍探索アルゴリズムが提案されており、距離計算回数を削減する\n",
    "\n",
    "### 参考URL\n",
    "\n",
    "- [k近傍法](https://ja.wikipedia.org/wiki/K近傍法)\n",
    "\n",
    "## k平均法(k-means clustering)\n",
    "\n",
    "- 教師なし学習のクラスタリング手法\n",
    "- データをk個のクラスタに分類\n",
    "    - クラスタリング : 特徴の似た者同士をグループ化\n",
    "- クラスタリングする手順\n",
    "    1. k個のクラスタ中心を決める(ランダム)\n",
    "    2. 各データ点に対し\n",
    "        1. 各クラスタ中心との距離を計算する\n",
    "        2. 最も距離が近いクラスタをデータ点に割り当てる\n",
    "    3. クラスタに割り当てられたデータより、各クラスタの中心点(平均ベクトル)を計算し、中心を更新する\n",
    "    4. クラスタの再割当てと、中心の更新を繰り返す(1〜3のループ)\n",
    "    \n",
    "### 注意点\n",
    "    \n",
    "- 中心の初期値を変えるとクラスタリング結果も変わりうる\n",
    "    - よって初期値を何回か変えて、クラスタリング結果が変わらないことを確認する必要がある\n",
    "- kの値を変えるとクラスタリング結果も変わる\n",
    "    - ドメインのナレッジによってkの値を決める（専門家の意見を仰ぐ場合もある)\n",
    "    - データサイエンティストの会合などでは、データ分析がAIエンジニアなどのデータ基盤を作れる人だけではビジネスができないとよく言われている\n",
    "        - 必ずデータを分析できる人と、自分たちが参入したいビジネスに精通する人とで協力してビッグデータ基盤とビジネスを紐づけないと行けないという話\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVHVcx1_H0cN"
   },
   "source": [
    "# 第６章：Appendix\n",
    "\n",
    "```レポート対象範囲外です```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HGMdGbcDH9Ns"
   },
   "source": [
    "# 第７章：サポートベクターマシン"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02_機械学習.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
