{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4fhror4LEgr"
   },
   "source": [
    "iStudy ACADEMY 現場で潰しが効くディープラーニング講座 視聴課題レポート 深層学習 前半1,2（講義動画と実装演習）\n",
    "\n",
    "# 深層学習 前半1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用ライブラリ\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークの全体像\n",
    "\n",
    "- ポイント\n",
    "  - 回帰や分類など手段があるが、求めたい出力を何か深く考え、入力値をよく考えて設計する。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認テスト\n",
    "\n",
    "- Q : ディープラーニングは、結局何をやろうとしているのか二行以内で述べよ。また、どの値の最適化が最終目的か。全て選べ。  \n",
    "①入力値[X] ②出力値[Y] ③重み[W]④バイアス[b] ⑤総入力[u] ⑥中間層入力[z] ⑦学習率[ρ]\n",
    "\n",
    "`考察`\n",
    "\n",
    "- よく考え選択された大量の入力値を用い、汎化性能が高いモデルを生成すること。またその出力値(予想値)を活用し、問題を解決することではないか。\n",
    "- 最適化された ②出力値[Y] を得ることが最終目的と考えられる。  \n",
    "パラメータ最適化の観点では、③重み[W]④バイアス[b] ⑦学習率[ρ] である。\n",
    "\n",
    "`参考)講師の考え`\n",
    "- 様々な考え方があるが(あってよい)、学習を通して誤差を最小にするネットワークを作成すること\n",
    "- 誤差E(w)を最小化するパラメータwを発見する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : 次のネットワークを紙にかけ。\n",
    "\n",
    "入力層 : 2ノード1層  \n",
    "中間層 : 3ノード2層  \n",
    "出力層 : 1ノード1層  \n",
    "\n",
    "`考察`\n",
    "\n",
    "<img src=\"./images/day1/section_0.png\" width=\"500px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erWaiWY6VPQ9"
   },
   "source": [
    "# Section1 : 入力層〜中間層\n",
    "\n",
    "- 入力層の入力 $x_i$ に重み $w_i$ がそれぞれ乗算し加算。バイアス $b$ も加算されたのち、中間層の入力 $u$ となる。\n",
    "- 重み $w_i$ は $x_i$ に対応し各層単位で存在する。\n",
    "- 重み $w_i$ と $b$ はネットワークのパラメータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認テスト\n",
    "\n",
    "- Q : 図式に動物分類の実例を入れてみよう。\n",
    "\n",
    "`考察`\n",
    "\n",
    "<img src=\"./images/day1/section_1.png\" width=\"500px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : この数式をPythonで書け。\n",
    "\n",
    "`考察`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "def calc_u(W, x, b):\n",
    "    return W @ x.T + b  # Python3.5以上の記法\n",
    "\n",
    "# テスト\n",
    "W = np.array([1,2,3])\n",
    "x = np.array([3,2,1])\n",
    "b = 1\n",
    "\n",
    "print(calc_u(W,x,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : 1-1のファイルから中間層の出力を定義しているソースを抜き出せ。(1_1_forward_propagation.ipynb)\n",
    "\n",
    "`考察`\n",
    "\n",
    "```python\n",
    "# 中間層出力\n",
    "z = functions.relu(u)\n",
    "print_vec(\"中間層出力\", z)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "\n",
    "[演習 1-1](./03_exercise/lesson_1/1_1_forward_propagation.ipynb)\n",
    "\n",
    "`考察`\n",
    "\n",
    "- numpyはベクトル・行列形式でデータ生成することに便利に利用できる\n",
    "- ネットワーク層を変化させる方法が具体的に理解できた。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c8XKJrfWVZvl"
   },
   "source": [
    "# Section2 : 活性化関数\n",
    "\n",
    "- 活性化関数によって、人間の脳を模したNN内の信号を伝える強さを決める。\n",
    "  - NNにおいて次の層への出力の大きさを決める非線形関数。\n",
    "  - 入力値によって、次の層への信号のON/OFFや強弱を定める働きを持つ。\n",
    "- 活性化関数は層によって分類できる。また使い方も違う。  \n",
    "※下記はスライド・更新の話と調査して得た情報をまとめたもの\n",
    "\n",
    "| 層 | 関数名 | 特徴 | 課題 |\n",
    "| - | - | - | - |\n",
    "| 中間 | ReLU関数 | 今最も使われている活性化関数。<br><u>勾配消失問題の回避</u>と<u>スパース化</u>に貢献することで良い成果をもたらしている。 | - |\n",
    "| 中間 | シグモイド関数（ロジスティック）関数 | 0〜1間を緩やかに変化する関数でステップ関数と比べ信号の強弱を伝達可能。<br>予想NN普及のきっかけとなった。 | 大きな値では出力変化が微少なため、<br>勾配消失問題を起こす事がある。 |\n",
    "| 中間 | ステップ関数 | 閾値を超えたら発火し出力は常に0か1。<br>パーセプトロン(NNの前身)で利用された。 | 0-1間を表現できず、<br>線形分離可能なもののみ学習可能。 |\n",
    "| 出力 | ソフトマックス関数 | 出力値の和が綺麗に100%(1.0)になる数字に変えてくれる関数。他クラス分類に都合が良い。 | - |\n",
    "| 出力 | 恒等写像 | 入力した値と同じ値を常にそのまま返す関数。 | - |\n",
    "| 出力 | シグモイド関数(ロジスティック）関数 | 二値分類で使用。 | - |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認テスト\n",
    "\n",
    "- Q : 線形と非線形の違いを図に書いて簡易に説明せよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- 線形は傾きが一定で変化せず、非線形は傾きが一定ではなく、変化する。\n",
    "\n",
    "<img src=\"./images/day1/section_2_1.png\" width=\"500px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : 配布されたソースコードより該当する箇所を抜き出せ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "```python\n",
    "# 中間層出力\n",
    "z = functions.sigmoid(u)\n",
    "print_vec(\"中間層出力\", z)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjGuhvuAVhMI"
   },
   "source": [
    "# Section3 : 出力層\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1 : 誤差関数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二乗誤差関数 : $\\displaystyle E_n^{(w)} = \\frac{1}{2}\\sum_{j=1}^l(y_j-d_j)^2 = \\frac{1}{2}||y-d||^2$\n",
    "- 一般的にクラス分類問題には誤差関数に二乗誤差は利用しません `【講師より】`\n",
    "  - softmax関数を利用したあとクロスエントロピーを利用、またはクロスエントロピーのみ利用する場合が多い `【独自調査】`\n",
    "    - 多クラス分類の誤差関数 : クロスエントロピー\n",
    "    - 二クラス分類の誤差関数 : 二値交差エントロピー\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認テスト\n",
    "\n",
    "- Q : なぜ、引き算でなく二乗するか答えよ。\n",
    "- Q : 下式の1/2はどういう意味を持つか述べよ。  \n",
    "$\\displaystyle E_n(w) = \\frac{1}{2}\\sum_{j=1}^l (y_j-d_j)^2 = \\frac{1}{2}||(y-d)||^2$\n",
    "\n",
    "`考察`\n",
    "\n",
    "- 絶対値を取ると微分不可能となるが、二乗することで簡単に符号をプラスに揃えることが出来る。  \n",
    "  例えば統計学において分散の計算が偏差の二乗を取ることと同様である。\n",
    "- 誤差を最小化する目的のため、式に $\\frac{1}{2}$ を掛けることに問題はない。  \n",
    "  微分した結果、指数の2が前に降り $\\frac{1}{2}$ と打ち消しあい式から消え、計算が簡易化される。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2 : 出力層の活性化関数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 出力層と中間層の違い\n",
    "  - 値の強弱\n",
    "    - 中間層 : 閾値の前後で信号の強弱を調整\n",
    "    - 出力層 : 信号の大きさ（比率）はそのままに変換\n",
    "  - 確率出力\n",
    "    - 分類問題の場合、出力層の出力は 0〜1 の範囲に限定し、総和を1とする必要がある\n",
    "  - 結論\n",
    "    - 出力層と中間層で利用される活性化関数が異なる\n",
    "- 出力層の活性化関数と誤差関数(一般的なもの)\n",
    "\n",
    "| 問題 | 出力層の活性化関数 | 誤差関数 |\n",
    "| - | - | - |\n",
    "| 回帰 | 恒等写像 | 二乗誤差関数 |\n",
    "| 二値分類 | シグモイド関数 | 二値交差エントロピー |\n",
    "| 他クラス分類 | ソフトマックス関数 | 交差エントロピー |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認テスト\n",
    "\n",
    "- Q : ①〜③の数式に該当するソースコードを示し、一行づつ処理の説明をさせよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- $f(i,u)$ ... ①\n",
    "\n",
    "```python\n",
    "def softmax(x):\n",
    "\n",
    "# 入力ベクトルの要素を示すインデックス i を引数として受け取らず、全て計算した結果をベクトルとして返す関数\n",
    "```\n",
    "\n",
    "- $e^{u_i}$ ... ②\n",
    "\n",
    "```python\n",
    "np.exp(x)\n",
    "\n",
    "# Numpyのexp()関数を利用し、xの個々に対しeのx乗を計算している\n",
    "```\n",
    "\n",
    "- $\\sum_{k=1}^{K}e^{u_k}$ ... ③\n",
    "\n",
    "```python\n",
    "np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# Numpyのsum()関数を利用し、xの個々に対しeのx乗を計算し全て加算している\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : ①〜②の数式に該当するソースコードを示し、一行づつ処理の説明をせよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- $E_n(w)$ ... ①\n",
    "\n",
    "```python\n",
    "def cross_entropy_error(d, y):\n",
    "\n",
    "# 引数dは教師データのエントロピー(one-hot-vectorでもOK),引数yはニューラルネットワークモデルの出力とし、交差エントロピーを計算して返す関数\n",
    "```\n",
    "\n",
    "- $-\\sum_{i=1}^{l}d_{i} \\log y_i$ ... ②\n",
    "\n",
    "```python\n",
    "return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size\n",
    "\n",
    "# np.log()へゼロを渡さないように 1e-7 の極小値を引数に加算している\n",
    "# ニューラルネットワークモデルの出力であるyの各要素は，各クラスの出現(推定)確率に対応付けており、softmax関数等で正規化されている前提である\n",
    "```\n",
    "\n",
    "## 参考文献・URL\n",
    "\n",
    "- [学習の種類と誤差関数 | Nishii's Notebook](http://bcl.sci.yamaguchi-u.ac.jp/~jun/notebook/keras/loss)\n",
    "- [出力層の活性化関数と誤差関数 - Wikipedia](https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0#%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AE%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0%E3%81%A8%E8%AA%A4%E5%B7%AE%E9%96%A2%E6%95%B0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section4 : 勾配降下法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基本的な考え方\n",
    "  - 誤差関数の値をより小さくする方向に重み $W$ 及びバイアス $b$ を更新し次の周(エポック)に反映する\n",
    "- 種類\n",
    "  - 勾配降下法\n",
    "    - パラメータ更新にあたり、全訓練データの平均誤差を取る。よって、計算に1エポックの全入力データが計算に利用されることとなる。\n",
    "  - 確率的勾配降下法(SGD)\n",
    "    - パラメータ更新にあたり、ランダムに抽出した入力データの誤差を利用する。よって以下のメリットがある。\n",
    "      - 計算コスト軽減\n",
    "      - 局所極小解に収束するリスク軽減\n",
    "      - 逐次計算するため、オンライン学習可能\n",
    "  - ミニバッチ勾配降下法\n",
    "    - パラメータ更新にあたり、ランダム分割した特定のデータ集合(ミニバッチ)の平均誤差を利用する。よって以下のメリットがある。\n",
    "      - **SGDのメリットを活かしたまま**、スレッド並列化やGPUを活用したSIMD並列化が可能\n",
    "- 重要用語\n",
    "  - 学習率 : 小さいと計算に時間が掛かり過ぎ、大きいと発散してしまう。\n",
    "  - 大域的極小解 : 次元数が高い関数において、傾きゼロの位置が復数存在する。その一番低い位置。  \n",
    "  計算上、より低い位置で傾きゼロを求められるにも関わらず、途中の谷で得られた傾きゼロの箇所を答えとみなしてしまう恐れがある。\n",
    "- 学習率の決定、収束性向上のためのアルゴリズム\n",
    "  - 復数の論文が発表されている　→　`それだけ重要ということ`\n",
    "    - Momentum\n",
    "    - AdaGrad\n",
    "    - Adadelta\n",
    "    - Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認テスト\n",
    "\n",
    "- Q : 該当するソースコードを探してみよう。\n",
    "\n",
    "$$\\displaystyle w^{(t+1)} = t^{(t)} - \\epsilon \\nabla E$$\n",
    "\n",
    "$$\\displaystyle \\nabla E = \\frac{\\partial E}{\\partial w} = [\\frac{\\partial E}{\\partial w_w} ... \\frac{\\partial E}{\\partial w_M}]$$\n",
    "\n",
    "`考察`\n",
    "\n",
    "```python\n",
    "grad = backward(x, d, z1, y)\n",
    "for key in ('W1', 'W2', 'b1', 'b2'):\n",
    "    network[key]  -= learning_rate * grad[key]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : オンライン学習とは何か２行でまとめよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- ユーザーの操作、環境の周囲から入る情報などリアルタイムで入る情報より、逐次訓練データとして投入し学習（パラメータ更新)を行う\n",
    "- 学習済みのモデルに対し、学習データを追加投入して学習させる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : この数式の意味を図に書いて説明せよ。\n",
    "\n",
    "$$\\displaystyle w^{(t+1)} = w^{(t)} - \\epsilon \\nabla E_t$$\n",
    "\n",
    "`解説`\n",
    "\n",
    "- 数式はミニバッチ勾配降下法の更新式。\n",
    "- $t$ 時点での誤差 $E$ の傾きに学習率を掛けて調整し、次回の $t+1$ 時点でのウェイト $w^{t+1}$ へ 現時点でのウェイト $w^{t}$ に引き算した結果を反映する。\n",
    "\n",
    "<img src=\"./images/day1/section_4.png\" width=\"500px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section5 : 誤差逆伝搬法\n",
    "\n",
    "- 誤差逆伝搬法 : 算出された誤差を、出力層側から順に微分し、前の層、前の層へと伝搬。  \n",
    "最小限の計算で各パラメータでの微分値を <u>解析的に</u> 計算する手法。\n",
    "- 誤差逆伝搬法のメリット : 計算結果(=誤差)から微分を逆算することで、不要な再起的計算を避けて微分を算出出来る。\n",
    "- 数値微分のデメリット : 各パラメータ $w_m$ それぞれについて $E(w_m + h)$ や $E(w_m - h)$ を計算するために、順伝搬の計算を繰り返し行う必要があり負荷が大きい\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認テスト\n",
    "\n",
    "- Q : 誤差逆伝搬法では不要な再帰的処理を避ける事が出来る。  \n",
    "逆に行った計算結果を保持しているソースコードを抽出せよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "```python\n",
    "# 誤差逆伝播\n",
    "def backward(x, d, z1, y):\n",
    "    print(\"\\n##### 誤差逆伝播開始 #####\")\n",
    "\n",
    "    grad = {}\n",
    "\n",
    "    W1, W2 = network['W1'], network['W2']\n",
    "    b1, b2 = network['b1'], network['b2']\n",
    "    #  出力層でのデルタ\n",
    "    delta2 = functions.d_sigmoid_with_loss(d, y)         ## シグモイド関数とクロスエントロピーの合成関数かつ導関数の値を計算\n",
    "    #  b2の勾配\n",
    "    grad['b2'] = np.sum(delta2, axis=0)                  ## delta2を利用\n",
    "    #  W2の勾配\n",
    "    grad['W2'] = np.dot(z1.T, delta2)                    ## delta2を利用\n",
    "    #  中間層でのデルタ\n",
    "    delta1 = np.dot(delta2, W2.T) * functions.d_relu(z1) ## delta2を利用 -> delta1を計算\n",
    "    # b1の勾配\n",
    "    grad['b1'] = np.sum(delta1, axis=0)                  ## delta1を利用\n",
    "    #  W1の勾配\n",
    "    grad['W1'] = np.dot(x.T, delta1)                     ## delta1を利用\n",
    "        \n",
    "    print_vec(\"偏微分_dE/du2\", delta2)\n",
    "    print_vec(\"偏微分_dE/du2\", delta1)\n",
    "\n",
    "    print_vec(\"偏微分_重み1\", grad[\"W1\"])\n",
    "    print_vec(\"偏微分_重み2\", grad[\"W2\"])\n",
    "    print_vec(\"偏微分_バイアス1\", grad[\"b1\"])\n",
    "    print_vec(\"偏微分_バイアス2\", grad[\"b2\"])\n",
    "\n",
    "    return grad\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : ２つの空欄に該当するソースコードを探せ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "$\\displaystyle \\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial u}$ -> `delta2 = functions.d_mean_squared_error(d, y)`\n",
    "\n",
    "$\\displaystyle \\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial u}\\frac{\\partial u}{\\partial  w_{ji}^{(2)} }$ -> `grad['W2'] = np.dot(z1.T, delta2)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "\n",
    "[演習 1-3](./03_exercise/lesson_1/1_3_stochastic_gradient_descent.ipynb)\n",
    "\n",
    "`考察`\n",
    "\n",
    "- 誤差逆伝播は順伝搬と逆伝搬での生起関数、導関数をあわせなければならない。\n",
    "- sigmoidより、reluのほうが収束が早い。\n",
    "- 訓練データである入力値の数値範囲の大きさが変わると収束が遅くなる。(大きくなる→収束に時間がかかる）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 深層学習 前半2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section0 : 深層学習全体像の復習\n",
    "\n",
    "### 確認テスト\n",
    "\n",
    "- Q : 連鎖率の原理を使い、 $\\frac{\\Delta z}{\\Delta x}$ を求めよ。\n",
    "$$z = t^2$$\n",
    "$$t = x + y$$\n",
    "\n",
    "`考察`\n",
    "\n",
    "$$ \\frac{\\Delta z}{\\Delta x} = \\frac{\\Delta z}{\\Delta t} \\cdot \\frac{\\Delta t}{\\Delta x} = 2t = 2(x + y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1 : 勾配消失問題\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1 活性化関数\n",
    "\n",
    "- シグモイド関数\n",
    "  - 大きな値では出力が微少となり、勾配消失問題を引き起こす。\n",
    "- ReLU関数\n",
    "  - 大きな値を大きな値として出力するため、勾配消失問題を起こしにくい。\n",
    "  - スパース化にも貢献する。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2 初期値の設定方法\n",
    "\n",
    "- Xavier\n",
    "  - 初期値設定の際に組み合わせる活性化関数\n",
    "    - ReLU関数\n",
    "    - シグモイド（ロジスティック）関数\n",
    "    - 双曲線正接関数（ハイパボリックタンジェント）\n",
    "  - 重みの要素を、前の層のノード数の平方根で除算した値  \n",
    "```python\n",
    "network['W1'] = np.random.randn(input_layer_size,    hidden_layer_1_size) / np.sqrt(input_layer_size)\n",
    "network['W2'] = np.random.randn(hidden_layer_1_size, hidden_layer_2_size) / np.sqrt(hidden_layer_1_size)\n",
    "network['W3'] = np.random.randn(hidden_layer_2_size, output_layer_size)   / np.sqrt(hidden_layer_2_size)\n",
    "```\n",
    "- He\n",
    "  - 初期値設定の際に組み合わせる活性化関数\n",
    "    - ReLU関数\n",
    "  - 重みの要素を、前の層のノード数の平方根で除算した値に対し **<u> $\\sqrt{2}$ をかけ合わせた値</u>**  \n",
    "```python\n",
    "network['W1'] = np.random.randn(input_layer_size,    hidden_layer_1_size) / np.sqrt(input_layer_size)    * np.sqrt(2)\n",
    "network['W2'] = np.random.randn(hidden_layer_1_size, hidden_layer_2_size) / np.sqrt(hidden_layer_1_size) * np.sqrt(2)\n",
    "network['W3'] = np.random.randn(hidden_layer_2_size, output_layer_size)   / np.sqrt(hidden_layer_2_size) * np.sqrt(2)\n",
    "```\n",
    "\n",
    "#### 調査\n",
    "\n",
    "- `np.random.randn(?, ?)` は乱数生成 -> 平均0、分散1（標準偏差1）の正規分布\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### randn ###\n",
      "[[ 1.39346479 -0.14445003 -0.55141908  0.15199789  0.65316397]\n",
      " [ 1.32418695  2.09656018  1.55469687  0.5370804  -0.19877668]\n",
      " [-0.47838576 -0.62186099 -0.41962215  0.84665493  1.1677464 ]]\n",
      "\n",
      "### xavier ###\n",
      "[[ 0.80451727 -0.08339826 -0.31836195  0.08775602  0.37710439]\n",
      " [ 0.76451969  1.21044958  0.89760466  0.31008351 -0.11476377]\n",
      " [-0.27619615 -0.35903161 -0.24226896  0.48881645  0.6741987 ]]\n",
      "\n",
      "### He ###\n",
      "[[ 1.13775924 -0.11794295 -0.45023179  0.12410576  0.53330615]\n",
      " [ 1.08119411  1.71183422  1.26940468  0.43852431 -0.16230048]\n",
      " [-0.39060034 -0.50774737 -0.34262005  0.69129085  0.95346094]]\n"
     ]
    }
   ],
   "source": [
    "# 確認\n",
    "tmpRandn  = np.random.randn(3,5)    # 入力層=3,中間層=5 のノード数とし、正規分布から3x5の行列(乱数)を生成\n",
    "tmpXavier = tmpRandn / np.sqrt(3)\n",
    "tmpHe     = tmpXavier * np.sqrt(2)\n",
    "print('\\n### randn ###')\n",
    "print(tmpRandn)\n",
    "print('\\n### xavier ###')\n",
    "print(tmpXavier)\n",
    "print('\\n### He ###')\n",
    "print(tmpHe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3 バッチ正規化\n",
    "\n",
    "- バッチ正規化とは\n",
    "  - 2015年に提唱された最近のものであり、最新のCNNの中にも用いられている\n",
    "  - ミニバッチ単位で、入力値のデータの偏りを抑制する手法\n",
    "- 使い所\n",
    "  - 活性化関数に値を渡す前後に、バッチ正規化の処理を行う層を加える\n",
    "- バッチ正規化層への入力値\n",
    "  - $u^{(u)} = w^{l}z^{l-1}+b^{l}$ または $z$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認テスト\n",
    "\n",
    "- Q : シグモイド関数を微分したとき、入力値が０の時に最大値を取る。その値として正しいものを選択肢から選べ。\n",
    "  - (1) 0.15\n",
    "  - (2) 0.25\n",
    "  - (3) 0.35\n",
    "  - (4) 0.45\n",
    "\n",
    "`考察`\n",
    "\n",
    "  - シグモイド関数 -> $sigmoid(x)$  \n",
    "  - シグモイド関数の導関数 -> $(1 - sigmoid(x)) * sigmoid(x)$  \n",
    "  - $sigmoid(0) = 0.5$ なので導関数の値は $(1 - 0.5) * 0.5 = 0.25$\n",
    "    - よって **(2) 0.25**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) =  0.5\n",
      "d_sigmoid(0) =  0.25\n"
     ]
    }
   ],
   "source": [
    "# 確認\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    dx = (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "    return dx\n",
    "\n",
    "print('sigmoid(0) = ', sigmoid(0))\n",
    "print('d_sigmoid(0) = ', d_sigmoid(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : 重みの初期値に０を設定すると、どのような問題が発生するか。簡潔に説明せよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- 誤差が大きくなり、誤差逆伝搬で一度に更新される誤差が小さくなるのでは。\n",
    "- 同一層の重みが同じ値で更新される可能性。\n",
    "\n",
    "`講師より`\n",
    "\n",
    "- **全ての値が同じ値で伝わるため、パラメータのチューニングが行われなくなる。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : 一般的に考えられるバッチ正規化の効果を２点挙げよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- 学習を速く進行させることができる（学習係数を大きくすることができる）\n",
    "- 過学習を抑制する(Dropout などの必要性を減らす)\n",
    "\n",
    "`講師より`\n",
    "\n",
    "- **計算の高速化**\n",
    "- **勾配消失が起きづらくなる**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習の結果と考察\n",
    "\n",
    "`実施結果`\n",
    "\n",
    "- [演習 2_2_1_vanishing_gradient](./03_exercise/lesson_2/2_2_1_vanishing_gradient.ipynb)\n",
    "    - 変更箇所は【レポート提出者変更】とコメントで囲って記載\n",
    "\n",
    "`考察`\n",
    "\n",
    "- sigmoid - gauss のNNでは学習が進まず正答率が向上しないが、活性化関数をReLUに変更する、または初期値設定をXavier/Heに変更することで、学習が行えるようになる。\n",
    "- 隠れ層の数を増加させても基本的には正答率(学習率)は向上しないが、ReLU - gauss に限り上昇する。\n",
    "- 活性化関数がReLUの場合、初期値設定をXavier/Heにすることで正答率(学習率)が上昇する。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 活性化関数 | 初期値設定 | 正答率 | **隠れ層2倍** | **隠れ層4倍** | 備考 |\n",
    "| - | - | - | - | - | - |\n",
    "| sigmoid | gauss  | <img src=\"./images/day2/01_sigmoid_gauss.png\" width=\"300px\" />  | <img src=\"./images/day2/01_sigmoid_gauss_2.png\" width=\"300px\" />  | <img src=\"./images/day2/01_sigmoid_gauss_4.png\" width=\"300px\" />  | 学習不能 |\n",
    "| sigmoid | Xavier | <img src=\"./images/day2/03_sigmoid_xavier.png\" width=\"300px\" /> | <img src=\"./images/day2/03_sigmoid_xavier_2.png\" width=\"300px\" /> | <img src=\"./images/day2/03_sigmoid_xavier_4.png\" width=\"300px\" /> | - |\n",
    "| sigmoid | He     | <img src=\"./images/day2/06_sigmoid_he.png\" width=\"300px\" />     | <img src=\"./images/day2/06_sigmoid_he_2.png\" width=\"300px\" />     | <img src=\"./images/day2/06_sigmoid_he_4.png\" width=\"300px\" />     | **課題:Xavier -> He** |\n",
    "| ReLU    | gauss  | <img src=\"./images/day2/02_relu_gauss.png\" width=\"300px\" />     | <img src=\"./images/day2/02_relu_gauss_2.png\" width=\"300px\" />     | <img src=\"./images/day2/02_relu_gauss_4.png\" width=\"300px\" />     | - |\n",
    "| ReLU    | Xavier | <img src=\"./images/day2/07_relu_xavier.png\" width=\"300px\" />    | <img src=\"./images/day2/07_relu_xavier_2.png\" width=\"300px\" />    | <img src=\"./images/day2/07_relu_xavier_4.png\" width=\"300px\" />    | **課題:He -> Xavier** |\n",
    "| ReLU    | He     | <img src=\"./images/day2/04_relu_he.png\" width=\"300px\" />        | <img src=\"./images/day2/04_relu_he_2.png\" width=\"300px\" />        | <img src=\"./images/day2/04_relu_he_4.png\" width=\"300px\" />        | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2 : 学習率最適化手法\n",
    "\n",
    "- 学習率の大小には長短所があり、一概に決めずらい。\n",
    "\n",
    "| 学習率 | 長所 | 短所 |\n",
    "| - | - | - |\n",
    "| 大きい | 収束が速い | 最適値が得られず発散する |\n",
    "| 小さい | 収束が遅い | 大域局所最適値に収束しづらい |\n",
    "\n",
    "- よって初期学習率を大きくし、徐々に学習率を小さくする。\n",
    "- パラメータ毎に学習率を変化させることも出来る。\n",
    "\n",
    "### 2-1 モメンタム\n",
    "\n",
    "- 手法\n",
    "  1. 誤差をパラメータで微分したものと学習率の積を減算\n",
    "  2. 現在の重みに前回の重みを減算した値と慣性: $\\mu$ の積を加算\n",
    "\n",
    "\n",
    "- メリット\n",
    "  - 大域的最適化を得られ、局所最適解を防げる。\n",
    "  - 谷間についてから最も低い位置(最適解)にいくまでの時間が速い。\n",
    "\n",
    "`講師より`\n",
    "- 講師の見てきた範囲では、 ハイパーパラメータ $\\mu$ は0.05 〜 0.09 の間であることが多い\n",
    "\n",
    "### 2-2 AdaGrad\n",
    "\n",
    "- 手法\n",
    "  1. 誤差をパラメータで微分したものと、再定義した学習率の積を減算する\n",
    "  2. 学習率には誤差の傾きを逆数にしたものを掛けるため、傾きが大きいほど以降の学習率が徐々に小さく算出される\n",
    "\n",
    "- メリット\n",
    "  - 勾配が緩やかな斜面に対して最適値に近づける。\n",
    "\n",
    "- 課題\n",
    "  - 学習率が徐々に小さくなるため鞍点問題を引き起こす傾向がある。\n",
    "\n",
    "### 2-3 RMSProp\n",
    "\n",
    "- 手法\n",
    "  - AdaGradの学習率係数に対して、前の情報をどれぐらい減衰して伝えるかを示すハイパーパラメータ $\\alpha$ を導入\n",
    "- メリット\n",
    "  - 局所的最適解にはならず、(AdaGradに比較して)大域的最適解が得られる。\n",
    "  - ハイパーパラメータの調整が必要な場合が少ない。\n",
    "\n",
    "### 2-4 Adam\n",
    "\n",
    "`もっともよく使われている最適化アルゴリズムである。`  \n",
    "`Adam も RMSprop の改良版であり、勾配に関しても以前の情報を指数的減衰させながら伝える。`\n",
    "\n",
    "- 手法\n",
    "  - 以下をそれぞれ孕んだ最適化手法\n",
    "    - モメンタム : 過去の勾配の指数関数的減衰平均\n",
    "    - RMSProp : 過去の勾配の二乗の指数関数的減衰平均\n",
    "- メリット\n",
    "  - モメンタム及びRMSPropの良い所取り\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認テスト\n",
    "\n",
    "- Q : モメンタム・AdaGrad・RMSPropの特徴をそれぞれ簡潔に説明せよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "| 学習率最適化手法 | 特徴 | | 備考 |\n",
    "| - | - | - |\n",
    "| モメンタム | 重みの更新に慣性 $\\mu$ を用いパラメータの更新を慣性的なものにする。<br>ただし $\\mu$ もハイパーパラメータである。<br>安定した収束が得られ、他の収束が早い最適化手法を差し置いて採用される場合がある。 | - |\n",
    "| AdaGrad | 各次元ごとに学習率を自動調整してくれる。ただし初期学習率を決める必要がある。 | 2011年にJohn Duchiらが提唱 | \n",
    "| RMSProp | AdaGradでは一度学習率が0に十分近くなってしまった次元に関しては、<br>まだ坂があってもほとんど更新されなくなってしまう。<br>その改善として、AdaGradでは過去の勾配情報を全て均等に考慮していたが、<br>勾配の2乗の指数移動平均を取るように変更し、より直近の勾配情報を優先して計算する改良が行われた。 | 2012年にTijmen Tielemanらが提唱 |\n",
    "\n",
    "#### 参考URL\n",
    "\n",
    "- [Optimizer : 深層学習における勾配法について](https://qiita.com/tokkuman/items/1944c00415d129ca0ee9)\n",
    "- [深層学習の最適化アルゴリズム](https://qiita.com/ZoneTsuyoshi/items/8ef6fa1e154d176e25b8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習の結果と考察\n",
    "\n",
    "`実施結果`\n",
    "\n",
    "- [演習 2_4_optimizer](./03_exercise/lesson_2/2_4_optimizer.ipynb)\n",
    "    - 変更箇所は【レポート提出者変更】とコメントで囲って記載\n",
    "\n",
    "`考察`\n",
    "\n",
    "- 学習率最適化手法として、Momentum/AdaGrad/RMSProp/Adamを利用、結果を確認した。\n",
    "- SGDでは正答率が向上しない **sigmoid - gauss** に対し、Momentum/AdaGradを利用しても、正答率は向上しなかった。\n",
    "- **sigmoid - gauss** を **ReLU - Xavier** へ変更した場合、Momentum等学習率最適化手法の適用で明らかな正答率上昇が見られた。\n",
    "- 微妙な差だが、正答率の上昇率は Momentum < AdaGrad < PMSProp < Adam の順と思われる。\n",
    "- **ReLU - Xavier** の適用後、さらにバッチ正規化も組み合わせることで、 **4% 〜 1%** 正答率の上昇が観測され、改善がなされた。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| 活性化関数 | 初期値設定 | 学習率最適化手法 | 正答率<br>(1)変更なし | 正答率<br>(2)ReLU/Xavierへ変更 | 正答率<br>(3)さらにバッチ正規化オン | 備考 |\n",
    "| - | - | - | - | - | - | - |\n",
    "| sigmoid | gauss | SGD      | <img src=\"./images/day2_2/1.png\" width=\"300px\" /> | <img src=\"./images/day2_2/1_relu_xavier.png\" width=\"300px\" /> | <img src=\"./images/day2_2/1_relu_xavier_batch.png\" width=\"300px\" /> | (1)正答率(トレーニング) = 0.1100 正答率(テスト) = 0.1135<br>(2)正答率(トレーニング) = 0.8900 正答率(テスト) = 0.8600<br>(3)正答率(トレーニング) = 0.9300 正答率(テスト) = 0.9097 |\n",
    "| sigmoid | gauss | Momentum | <img src=\"./images/day2_2/2.png\" width=\"300px\" /> | <img src=\"./images/day2_2/2_relu_xavier.png\" width=\"300px\" /> | <img src=\"./images/day2_2/2_relu_xavier_batch.png\" width=\"300px\" /> | (1)正答率(トレーニング) = 0.1100 正答率(テスト) = 0.1135<br>(2)正答率(トレーニング) = 0.9500 正答率(テスト) = 0.9305<br>(3)正答率(トレーニング) = 0.9600 正答率(テスト) = 0.9574 |\n",
    "| sigmoid | gauss | AdaGrad  | <img src=\"./images/day2_2/3.png\" width=\"300px\" /> | <img src=\"./images/day2_2/3_relu_xavier.png\" width=\"300px\" /> | <img src=\"./images/day2_2/3_relu_xavier_batch.png\" width=\"300px\" /> | (1)正答率(トレーニング) = 0.0800 正答率(テスト) = 0.1135<br>(2)正答率(トレーニング) = 0.9600 正答率(テスト) = 0.9297<br>(3)正答率(トレーニング) = 0.9900 正答率(テスト) = 0.9506 |\n",
    "| sigmoid | gauss | RMSProp  | <img src=\"./images/day2_2/4.png\" width=\"300px\" /> | <img src=\"./images/day2_2/4_relu_xavier.png\" width=\"300px\" /> | <img src=\"./images/day2_2/4_relu_xavier_batch.png\" width=\"300px\" /> | (1)正答率(トレーニング) = 0.9900 正答率(テスト) = 0.9493<br>(2)正答率(トレーニング) = 0.9500 正答率(テスト) = 0.9390<br>(3)正答率(トレーニング) = 0.9800 正答率(テスト) = 0.9571 |\n",
    "| sigmoid | gauss | Adam     | <img src=\"./images/day2_2/5.png\" width=\"300px\" /> | <img src=\"./images/day2_2/5_relu_xavier.png\" width=\"300px\" /> | <img src=\"./images/day2_2/5_relu_xavier_batch.png\" width=\"300px\" /> | (1)正答率(トレーニング) = 0.9700 正答率(テスト) = 0.9441<br>(2)正答率(トレーニング) = 0.9900 正答率(テスト) = 0.9510<br>(3)正答率(トレーニング) = 0.9600 正答率(テスト) = 0.9633 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3 : 過学習\n",
    "\n",
    "- テスト誤差と訓練誤差とで学習曲線が乖離\n",
    "  - 特定の訓練サンプルに対して、適合しすぎてしまっている\n",
    "  - 汎化性能が低い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正則化とは\n",
    "\n",
    "- ネットワークの自由度(層数、ノード数、パラメータの値など）を制約すること\n",
    "  - 過学習を抑えることに貢献する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認テスト\n",
    "\n",
    "- Q : 機械学習で使われる線形モデル（線形回帰、主成分分析など）の正則化は、モデルの重みを制限することで可能となる。  \n",
    "前述の線形モデルの正則化手法の中にリッジ回帰という手法があり、その特徴として正しいものを選択しなさい。\n",
    "  - (a) ハイパーパラメータを大きな値に設定すると、すべての重みが限りなく０に近づく。\n",
    "  - (b) ハイパーパラメータを０に設定すると、非線形回帰となる。\n",
    "  - (c) バイアス項についても、正則化される。\n",
    "  - (d) リッジ回帰の場合、隠れ層に対して正則化項を加える。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- (a) L2正則化(リッジ回帰) : `正解`\n",
    "- (b) 非線形回帰にはならない\n",
    "- (c) バイアス項について正則化されない\n",
    "- (d) 誤差関数に関して正則化項を加える\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 L1正則化、L2正則化\n",
    "\n",
    "- Weight decay (荷重減衰)\n",
    "  - 学習させていくと、重みにばらつきが発生する。\n",
    "  - 重みが大きい値は重要な値であるが、過学習が発生する原因にもなる。\n",
    "  - 誤差関数に正則化項を加算し重みを抑制する。\n",
    "    - 重みを抑制することで過学習を緩和する。\n",
    "- 計算式\n",
    "  - $E_n(w) + \\frac{1}{p}\\lambda||x||_p$ : 誤差関数にpノルムを加える\n",
    "  - $||x||_p = (|x_1|^p + ... + |x_n|^p)^{\\frac{1}{p}}$ : pノルムの計算\n",
    "    - $p = 1$ の場合、L1正則化と呼ぶ\n",
    "    - $p = 2$ の場合、L2正則化と呼ぶ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認テスト\n",
    "\n",
    "- Q : 下図について、L1正則化を表しているグラフはどちらか答えよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- 左図 : Ridge推定量 : 縮小推定 : L2正則化\n",
    "- 右図 : Lasso推定量 : スパース推定 : **L1正則化** <- よって右図がL1正則化のグラフ\n",
    "\n",
    "`講師より`\n",
    "\n",
    "- 図中の同心円は誤差の等値線を表している。\n",
    "- 図中の丸・三角のエリアは誤差項と正則化項の接点を表している。\n",
    "- 図が混乱や疑問を招くようであれば、正則化の本質さえつかめていれば良い。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 ドロップアウト\n",
    "\n",
    "- 過学習の正則化の問題として、対象のノードが多い\n",
    "- ドロップアウト\n",
    "  - 正則化手法の中で一番使われている\n",
    "  - CNNでも多く使われて重宝されている手法\n",
    "- 手法\n",
    "  - ランダムにノードを削除(いないものと)して学習させる\n",
    "  - データ量を変化させずに、異なるモデルを学習させているとみなせる（アンサンブル学習）\n",
    "\n",
    "`講師より`\n",
    "\n",
    "- Tensorflow, Chainer 等機械学習用ライブラリにより層でドロップアウト、ノード単位でドロップアウトするのか肌感覚的に違う可能性がある。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習の結果と考察\n",
    "\n",
    "\n",
    "`実施結果`\n",
    "\n",
    "- [演習 2_5_overfitting](./03_exercise/lesson_2/2_5_overfitting.ipynb)\n",
    "    - 変更箇所は【レポート提出者変更】とコメントで囲って記載\n",
    "\n",
    "`考察`\n",
    "\n",
    "```WRITE DOC HERE!!!```  \n",
    "```WRITE DOC HERE!!!```  \n",
    "```WRITE DOC HERE!!!```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 正則化       | 変更点 | graph | - | - | - | - | 備考 |\n",
    "| -           | -     | - | - | - | - | - | - |\n",
    "| overfitting | -     | <img src=\"./images/day2_3/1.png\" width=\"300px\" /> | - | - | - | - | - |\n",
    "| L2          | weight_decay_lambda | <img src=\"./images/day2_3/2_L2_weight_0.01.png\" width=\"300px\" /><br>0.01 | <img src=\"./images/day2_3/2_L2_weight_0.1.png\" width=\"300px\" /><br>0.1 | <img src=\"./images/day2_3/2_L2_weight_0.2.png\" width=\"300px\" /><br>0.2 | <img src=\"./images/day2_3/2_L2_weight_0.3.png\" width=\"300px\" /><br>0.3 | <img src=\"./images/day2_3/2_L2_weight_0.5.png\" width=\"300px\" /><br>0.5 | - |\n",
    "| L1          | weight_decay_lambda | <img src=\"./images/day2_3/3_L1_weight_0.003.png\" width=\"300px\" /><br>0.003 | <img src=\"./images/day2_3/3_L1_weight_0.004.png\" width=\"300px\" /><br>0.004 | <img src=\"./images/day2_3/3_L1_weight_0.005.png\" width=\"300px\" /><br>0.005 | <img src=\"./images/day2_3/3_L1_weight_0.006.png\" width=\"300px\" /><br>0.006 | <img src=\"./images/day2_3/3_L1_weight_0.007.png\" width=\"300px\" /><br>0.007 | - |\n",
    "| dropout     | optimizer<br>-<br>dropout_radio | <img src=\"./images/day2_3/4_SGD_dropout_0.05.png\" width=\"300px\" /><br>SGD - 0.05 | <img src=\"./images/day2_3/4_SGD_dropout_0.1.png\" width=\"300px\" /><br>SGD - 0.1 | <img src=\"./images/day2_3/4_SGD_dropout_0.15.png\" width=\"300px\" /><br>SGD - 0.15 | <img src=\"./images/day2_3/4_SGD_dropout_0.2.png\" width=\"300px\" /><br>SGD - 0.2 | <img src=\"./images/day2_3/4_SGD_dropout_0.25.png\" width=\"300px\" /><br>SGD - 0.25 | - |\n",
    "| - | - | <img src=\"./images/day2_3/5_Momentum_dropout_0.05.png\" width=\"300px\" /><br>Momentum - 0.05 | <img src=\"./images/day2_3/5_Momentum_dropout_0.1.png\" width=\"300px\" /><br>Momentum - 0.1 | <img src=\"./images/day2_3/5_Momentum_dropout_0.15.png\" width=\"300px\" /><br>Momentum - 0.15 | <img src=\"./images/day2_3/5_Momentum_dropout_0.2.png\" width=\"300px\" /><br>Momentum - 0.2 | <img src=\"./images/day2_3/5_Momentum_dropout_0.25.png\" width=\"300px\" /><br>Momentum - 0.25 | - |\n",
    "| - | - | <img src=\"./images/day2_3/6_AdaGrad_dropout_0.05.png\" width=\"300px\" /><br>AdaGrad - 0.05 | <img src=\"./images/day2_3/6_AdaGrad_dropout_0.1.png\" width=\"300px\" /><br>AdaGrad - 0.1 | <img src=\"./images/day2_3/6_AdaGrad_dropout_0.15.png\" width=\"300px\" /><br>AdaGrad - 0.15 | <img src=\"./images/day2_3/6_AdaGrad_dropout_0.2.png\" width=\"300px\" /><br>AdaGrad - 0.2 | <img src=\"./images/day2_3/6_AdaGrad_dropout_0.25.png\" width=\"300px\" /><br>AdaGrad - 0.25 | - |\n",
    "| - | - | <img src=\"./images/day2_3/7_Adam_dropout_0.05.png\" width=\"300px\" /><br>Adam - 0.05 | <img src=\"./images/day2_3/7_Adam_dropout_0.1.png\" width=\"300px\" /><br>Adam - 0.1 | <img src=\"./images/day2_3/7_Adam_dropout_0.15.png\" width=\"300px\" /><br>Adam - 0.15 | <img src=\"./images/day2_3/7_Adam_dropout_0.2.png\" width=\"300px\" /><br>Adam - 0.2 | <img src=\"./images/day2_3/7_Adam_dropout_0.25.png\" width=\"300px\" /><br>Adam - 0.25 | - |\n",
    "| dropout+L1  | dropout_radio | <img src=\"./images/day2_3/8_dropout_L1_0.04.png\" width=\"300px\" /><br>L1 - 0.04 | <img src=\"./images/day2_3/8_dropout_L1_0.06.png\" width=\"300px\" /><br>L1 - 0.06 | <img src=\"./images/day2_3/8_dropout_L1_0.08.png\" width=\"300px\" /><br>L1 - 0.08 | <img src=\"./images/day2_3/8_dropout_L1_0.10.png\" width=\"300px\" /><br>L1 - 0.10 | <img src=\"./images/day2_3/8_dropout_L1_0.12.png\" width=\"300px\" /><br>L1 - 0.12 | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section4 : 畳み込みニューラルネットワークの概念\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習の結果と考察\n",
    "\n",
    "`実施結果`\n",
    "\n",
    "- [演習 2_6_simple_convolution_network](./03_exercise/lesson_2/2_6_simple_convolution_network.ipynb)\n",
    "    - 変更箇所は【レポート提出者変更】とコメントで囲って記載\n",
    "\n",
    "`考察`\n",
    "\n",
    "```WRITE DOC HERE!!!```  \n",
    "```WRITE DOC HERE!!!```  \n",
    "```WRITE DOC HERE!!!```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section5 : 最新のCNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03_深層学習_前半.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
