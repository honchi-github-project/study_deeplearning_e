{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4fhror4LEgr"
   },
   "source": [
    "iStudy ACADEMY 現場で潰しが効くディープラーニング講座 視聴課題レポート 深層学習 前半1,2（講義動画と実装演習）\n",
    "\n",
    "# 深層学習 前半1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用ライブラリ\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークの全体像\n",
    "\n",
    "- ポイント\n",
    "  - 回帰や分類など手段があるが、求めたい出力を何か深く考え、入力値をよく考えて設計する。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認テスト\n",
    "\n",
    "- Q : ディープラーニングは、結局何をやろうとしているのか二行以内で述べよ。また、どの値の最適化が最終目的か。全て選べ。  \n",
    "①入力値[X] ②出力値[Y] ③重み[W]④バイアス[b] ⑤総入力[u] ⑥中間層入力[z] ⑦学習率[ρ]\n",
    "\n",
    "`考察`\n",
    "\n",
    "- よく考え選択された大量の入力値を用い、汎化性能が高いモデルを生成すること。またその出力値(予想値)を活用し、問題を解決することではないか。\n",
    "- 最適化された ②出力値[Y] を得ることが最終目的と考えられる。  \n",
    "パラメータ最適化の観点では、③重み[W]④バイアス[b] ⑦学習率[ρ] である。\n",
    "\n",
    "`参考)講師の考え`\n",
    "- 様々な考え方があるが(あってよい)、学習を通して誤差を最小にするネットワークを作成すること\n",
    "- 誤差E(w)を最小化するパラメータwを発見する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : 次のネットワークを紙にかけ。\n",
    "\n",
    "入力層 : 2ノード1層  \n",
    "中間層 : 3ノード2層  \n",
    "出力層 : 1ノード1層  \n",
    "\n",
    "`考察`\n",
    "\n",
    "<img src=\"./images/day1/section_0.png\" width=\"500px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erWaiWY6VPQ9"
   },
   "source": [
    "# Section1 : 入力層〜中間層\n",
    "\n",
    "- 入力層の入力 $x_i$ に重み $w_i$ がそれぞれ乗算し加算。バイアス $b$ も加算されたのち、中間層の入力 $u$ となる。\n",
    "- 重み $w_i$ は $x_i$ に対応し各層単位で存在する。\n",
    "- 重み $w_i$ と $b$ はネットワークのパラメータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認テスト\n",
    "\n",
    "- Q : 図式に動物分類の実例を入れてみよう。\n",
    "\n",
    "`考察`\n",
    "\n",
    "<img src=\"./images/day1/section_1.png\" width=\"500px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : この数式をPythonで書け。\n",
    "\n",
    "`考察`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "def calc_u(W, x, b):\n",
    "    return W @ x.T + b  # Python3.5以上の記法\n",
    "\n",
    "# テスト\n",
    "W = np.array([1,2,3])\n",
    "x = np.array([3,2,1])\n",
    "b = 1\n",
    "\n",
    "print(calc_u(W,x,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : 1-1のファイルから中間層の出力を定義しているソースを抜き出せ。(1_1_forward_propagation.ipynb)\n",
    "\n",
    "`考察`\n",
    "\n",
    "```python\n",
    "# 中間層出力\n",
    "z = functions.relu(u)\n",
    "print_vec(\"中間層出力\", z)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "\n",
    "[演習 1-1](./03_exercise/lesson_1/1_1_forward_propagation.ipynb)\n",
    "\n",
    "`考察`\n",
    "\n",
    "- numpyはベクトル・行列形式でデータ生成することに便利に利用できる\n",
    "- ネットワーク層を変化させる方法が具体的に理解できた。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c8XKJrfWVZvl"
   },
   "source": [
    "# Section2 : 活性化関数\n",
    "\n",
    "- 活性化関数によって、人間の脳を模したNN内の信号を伝える強さを決める。\n",
    "  - NNにおいて次の層への出力の大きさを決める非線形関数。\n",
    "  - 入力値によって、次の層への信号のON/OFFや強弱を定める働きを持つ。\n",
    "- 活性化関数は層によって分類できる。また使い方も違う。  \n",
    "※下記はスライド・更新の話と調査して得た情報をまとめたもの\n",
    "\n",
    "| 層 | 関数名 | 特徴 | 課題 |\n",
    "| - | - | - | - |\n",
    "| 中間 | ReLU関数 | 今最も使われている活性化関数。<br><u>勾配消失問題の回避</u>と<u>スパース化</u>に貢献することで良い成果をもたらしている。 | - |\n",
    "| 中間 | シグモイド関数（ロジスティック）関数 | 0〜1間を緩やかに変化する関数でステップ関数と比べ信号の強弱を伝達可能。<br>予想NN普及のきっかけとなった。 | 大きな値では出力変化が微少なため、<br>勾配消失問題を起こす事がある。 |\n",
    "| 中間 | ステップ関数 | 閾値を超えたら発火し出力は常に0か1。<br>パーセプトロン(NNの前身)で利用された。 | 0-1間を表現できず、<br>線形分離可能なもののみ学習可能。 |\n",
    "| 出力 | ソフトマックス関数 | 出力値の和が綺麗に100%(1.0)になる数字に変えてくれる関数。他クラス分類に都合が良い。 | - |\n",
    "| 出力 | 恒等写像 | 入力した値と同じ値を常にそのまま返す関数。 | - |\n",
    "| 出力 | シグモイド関数(ロジスティック）関数 | 二値分類で使用。 | - |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認テスト\n",
    "\n",
    "- Q : 線形と非線形の違いを図に書いて簡易に説明せよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- 線形は傾きが一定で変化せず、非線形は傾きが一定ではなく、変化する。\n",
    "\n",
    "<img src=\"./images/day1/section_2_1.png\" width=\"500px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : 配布されたソースコードより該当する箇所を抜き出せ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "```python\n",
    "# 中間層出力\n",
    "z = functions.sigmoid(u)\n",
    "print_vec(\"中間層出力\", z)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjGuhvuAVhMI"
   },
   "source": [
    "# Section3 : 出力層\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1 : 誤差関数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二乗誤差関数 : $\\displaystyle E_n^{(w)} = \\frac{1}{2}\\sum_{j=1}^l(y_j-d_j)^2 = \\frac{1}{2}||y-d||^2$\n",
    "- 一般的にクラス分類問題には誤差関数に二乗誤差は利用しません `【講師より】`\n",
    "  - softmax関数を利用したあとクロスエントロピーを利用、またはクロスエントロピーのみ利用する場合が多い `【独自調査】`\n",
    "    - 多クラス分類の誤差関数 : クロスエントロピー\n",
    "    - 二クラス分類の誤差関数 : 二値交差エントロピー\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認テスト\n",
    "\n",
    "- Q : なぜ、引き算でなく二乗するか答えよ。\n",
    "- Q : 下式の1/2はどういう意味を持つか述べよ。  \n",
    "$\\displaystyle E_n(w) = \\frac{1}{2}\\sum_{j=1}^l (y_j-d_j)^2 = \\frac{1}{2}||(y-d)||^2$\n",
    "\n",
    "`考察`\n",
    "\n",
    "- 絶対値を取ると微分不可能となるが、二乗することで簡単に符号をプラスに揃えることが出来る。  \n",
    "  例えば統計学において分散の計算が偏差の二乗を取ることと同様である。\n",
    "- 誤差を最小化する目的のため、式に $\\frac{1}{2}$ を掛けることに問題はない。  \n",
    "  微分した結果、指数の2が前に降り $\\frac{1}{2}$ と打ち消しあい式から消え、計算が簡易化される。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2 : 出力層の活性化関数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 出力層と中間層の違い\n",
    "  - 値の強弱\n",
    "    - 中間層 : 閾値の前後で信号の強弱を調整\n",
    "    - 出力層 : 信号の大きさ（比率）はそのままに変換\n",
    "  - 確率出力\n",
    "    - 分類問題の場合、出力層の出力は 0〜1 の範囲に限定し、総和を1とする必要がある\n",
    "  - 結論\n",
    "    - 出力層と中間層で利用される活性化関数が異なる\n",
    "- 出力層の活性化関数と誤差関数(一般的なもの)\n",
    "\n",
    "| 問題 | 出力層の活性化関数 | 誤差関数 |\n",
    "| - | - | - |\n",
    "| 回帰 | 恒等写像 | 二乗誤差関数 |\n",
    "| 二値分類 | シグモイド関数 | 二値交差エントロピー |\n",
    "| 他クラス分類 | ソフトマックス関数 | 交差エントロピー |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認テスト\n",
    "\n",
    "- Q : ①〜③の数式に該当するソースコードを示し、一行づつ処理の説明をさせよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- $f(i,u)$ ... ①\n",
    "\n",
    "```python\n",
    "def softmax(x):\n",
    "\n",
    "# 入力ベクトルの要素を示すインデックス i を引数として受け取らず、全て計算した結果をベクトルとして返す関数\n",
    "```\n",
    "\n",
    "- $e^{u_i}$ ... ②\n",
    "\n",
    "```python\n",
    "np.exp(x)\n",
    "\n",
    "# Numpyのexp()関数を利用し、xの個々に対しeのx乗を計算している\n",
    "```\n",
    "\n",
    "- $\\sum_{k=1}^{K}e^{u_k}$ ... ③\n",
    "\n",
    "```python\n",
    "np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# Numpyのsum()関数を利用し、xの個々に対しeのx乗を計算し全て加算している\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : ①〜②の数式に該当するソースコードを示し、一行づつ処理の説明をせよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- $E_n(w)$ ... ①\n",
    "\n",
    "```python\n",
    "def cross_entropy_error(d, y):\n",
    "\n",
    "# 引数dは教師データのエントロピー(one-hot-vectorでもOK),引数yはニューラルネットワークモデルの出力とし、交差エントロピーを計算して返す関数\n",
    "```\n",
    "\n",
    "- $-\\sum_{i=1}^{l}d_{i} \\log y_i$ ... ②\n",
    "\n",
    "```python\n",
    "return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size\n",
    "\n",
    "# np.log()へゼロを渡さないように 1e-7 の極小値を引数に加算している\n",
    "# ニューラルネットワークモデルの出力であるyの各要素は，各クラスの出現(推定)確率に対応付けており、softmax関数等で正規化されている前提である\n",
    "```\n",
    "\n",
    "## 参考文献・URL\n",
    "\n",
    "- [学習の種類と誤差関数 | Nishii's Notebook](http://bcl.sci.yamaguchi-u.ac.jp/~jun/notebook/keras/loss)\n",
    "- [出力層の活性化関数と誤差関数 - Wikipedia](https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0#%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AE%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0%E3%81%A8%E8%AA%A4%E5%B7%AE%E9%96%A2%E6%95%B0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section4 : 勾配降下法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基本的な考え方\n",
    "  - 誤差関数の値をより小さくする方向に重み $W$ 及びバイアス $b$ を更新し次の周(エポック)に反映する\n",
    "- 種類\n",
    "  - 勾配降下法\n",
    "    - パラメータ更新にあたり、全訓練データの平均誤差を取る。よって、計算に1エポックの全入力データが計算に利用されることとなる。\n",
    "  - 確率的勾配降下法(SGD)\n",
    "    - パラメータ更新にあたり、ランダムに抽出した入力データの誤差を利用する。よって以下のメリットがある。\n",
    "      - 計算コスト軽減\n",
    "      - 局所極小解に収束するリスク軽減\n",
    "      - 逐次計算するため、オンライン学習可能\n",
    "  - ミニバッチ勾配降下法\n",
    "    - パラメータ更新にあたり、ランダム分割した特定のデータ集合(ミニバッチ)の平均誤差を利用する。よって以下のメリットがある。\n",
    "      - **SGDのメリットを活かしたまま**、スレッド並列化やGPUを活用したSIMD並列化が可能\n",
    "- 重要用語\n",
    "  - 学習率 : 小さいと計算に時間が掛かり過ぎ、大きいと発散してしまう。\n",
    "  - 大域的極小解 : 次元数が高い関数において、傾きゼロの位置が復数存在する。その一番低い位置。  \n",
    "  計算上、より低い位置で傾きゼロを求められるにも関わらず、途中の谷で得られた傾きゼロの箇所を答えとみなしてしまう恐れがある。\n",
    "- 学習率の決定、収束性向上のためのアルゴリズム\n",
    "  - 復数の論文が発表されている　→　`それだけ重要ということ`\n",
    "    - Momentum\n",
    "    - AdaGrad\n",
    "    - Adadelta\n",
    "    - Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認テスト\n",
    "\n",
    "- Q : 該当するソースコードを探してみよう。\n",
    "\n",
    "$$\\displaystyle w^{(t+1)} = t^{(t)} - \\epsilon \\nabla E$$\n",
    "\n",
    "$$\\displaystyle \\nabla E = \\frac{\\partial E}{\\partial w} = [\\frac{\\partial E}{\\partial w_w} ... \\frac{\\partial E}{\\partial w_M}]$$\n",
    "\n",
    "`考察`\n",
    "\n",
    "```python\n",
    "grad = backward(x, d, z1, y)\n",
    "for key in ('W1', 'W2', 'b1', 'b2'):\n",
    "    network[key]  -= learning_rate * grad[key]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : オンライン学習とは何か２行でまとめよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- ユーザーの操作、環境の周囲から入る情報などリアルタイムで入る情報より、逐次訓練データとして投入し学習（パラメータ更新)を行う\n",
    "- 学習済みのモデルに対し、学習データを追加投入して学習させる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : この数式の意味を図に書いて説明せよ。\n",
    "\n",
    "$$\\displaystyle w^{(t+1)} = w^{(t)} - \\epsilon \\nabla E_t$$\n",
    "\n",
    "`解説`\n",
    "\n",
    "- 数式はミニバッチ勾配降下法の更新式。\n",
    "- $t$ 時点での誤差 $E$ の傾きに学習率を掛けて調整し、次回の $t+1$ 時点でのウェイト $w^{t+1}$ へ 現時点でのウェイト $w^{t}$ に引き算した結果を反映する。\n",
    "\n",
    "<img src=\"./images/day1/section_4.png\" width=\"500px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section5 : 誤差逆伝搬法\n",
    "\n",
    "- 誤差逆伝搬法 : 算出された誤差を、出力層側から順に微分し、前の層、前の層へと伝搬。  \n",
    "最小限の計算で各パラメータでの微分値を <u>解析的に</u> 計算する手法。\n",
    "- 誤差逆伝搬法のメリット : 計算結果(=誤差)から微分を逆算することで、不要な再起的計算を避けて微分を算出出来る。\n",
    "- 数値微分のデメリット : 各パラメータ $w_m$ それぞれについて $E(w_m + h)$ や $E(w_m - h)$ を計算するために、順伝搬の計算を繰り返し行う必要があり負荷が大きい\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認テスト\n",
    "\n",
    "- Q : 誤差逆伝搬法では不要な再帰的処理を避ける事が出来る。  \n",
    "逆に行った計算結果を保持しているソースコードを抽出せよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "```python\n",
    "# 誤差逆伝播\n",
    "def backward(x, d, z1, y):\n",
    "    print(\"\\n##### 誤差逆伝播開始 #####\")\n",
    "\n",
    "    grad = {}\n",
    "\n",
    "    W1, W2 = network['W1'], network['W2']\n",
    "    b1, b2 = network['b1'], network['b2']\n",
    "    #  出力層でのデルタ\n",
    "    delta2 = functions.d_sigmoid_with_loss(d, y)         ## シグモイド関数とクロスエントロピーの合成関数かつ導関数の値を計算\n",
    "    #  b2の勾配\n",
    "    grad['b2'] = np.sum(delta2, axis=0)                  ## delta2を利用\n",
    "    #  W2の勾配\n",
    "    grad['W2'] = np.dot(z1.T, delta2)                    ## delta2を利用\n",
    "    #  中間層でのデルタ\n",
    "    delta1 = np.dot(delta2, W2.T) * functions.d_relu(z1) ## delta2を利用 -> delta1を計算\n",
    "    # b1の勾配\n",
    "    grad['b1'] = np.sum(delta1, axis=0)                  ## delta1を利用\n",
    "    #  W1の勾配\n",
    "    grad['W1'] = np.dot(x.T, delta1)                     ## delta1を利用\n",
    "        \n",
    "    print_vec(\"偏微分_dE/du2\", delta2)\n",
    "    print_vec(\"偏微分_dE/du2\", delta1)\n",
    "\n",
    "    print_vec(\"偏微分_重み1\", grad[\"W1\"])\n",
    "    print_vec(\"偏微分_重み2\", grad[\"W2\"])\n",
    "    print_vec(\"偏微分_バイアス1\", grad[\"b1\"])\n",
    "    print_vec(\"偏微分_バイアス2\", grad[\"b2\"])\n",
    "\n",
    "    return grad\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : ２つの空欄に該当するソースコードを探せ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "$\\displaystyle \\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial u}$ -> `delta2 = functions.d_mean_squared_error(d, y)`\n",
    "\n",
    "$\\displaystyle \\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial u}\\frac{\\partial u}{\\partial  w_{ji}^{(2)} }$ -> `grad['W2'] = np.dot(z1.T, delta2)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演習\n",
    "\n",
    "[演習 1-3](./03_exercise/lesson_1/1_3_stochastic_gradient_descent.ipynb)\n",
    "\n",
    "`考察`\n",
    "\n",
    "- 誤差逆伝播は順伝搬と逆伝搬での生起関数、導関数をあわせなければならない。\n",
    "- sigmoidより、reluのほうが収束が早い。\n",
    "- 訓練データである入力値の数値範囲の大きさが変わると収束が遅くなる。(大きくなる→収束に時間がかかる）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 深層学習 前半2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1 : 勾配消失問題\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2 : 学習率最適化手法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3 : 過学習\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section4 : 畳み込みニューラルネットワークの概念\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section5 : 最新のCNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03_深層学習_前半.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
