{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4fhror4LEgr"
   },
   "source": [
    "iStudy ACADEMY 現場で潰しが効くディープラーニング講座 視聴課題レポート 深層学習 後半1,2（講義動画と実装演習）\n",
    "<div style=\"text-align: right;\">【レポート作成者】S.Honda</div>\n",
    "\n",
    "# 深層学習 後半1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 再帰型ニューラルネットワークについて\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1 : 再帰型ニューラルネットワークの概念\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erWaiWY6VPQ9"
   },
   "source": [
    "### 1-1 : RNN全体像\n",
    "\n",
    "- RNNとは\n",
    "  - RNN (Recurrent Neural Network : 再帰型ニューラルネットワーク) は、ニューラルネットワークを拡張し時系列データを扱えるようにしたもの\n",
    "- 時系列データとは\n",
    "  - 時間的順序を追って一定間隔ごとに観察され、しかも相互に統計的依存関係が認められるようなデータの系列\n",
    "    - ある時刻の値は、以前の時刻の変化の延長上\n",
    "    - ある時間の経過とともに値が変化していくようなデータ\n",
    "  - データの例\n",
    "    - 音声データ, テキストデータ, 株価データなど\n",
    "    - 店舗の日次売上データ, ホームページのアクセス数履歴, 工場設備のセンサデータなどなど\n",
    "  - 関心の対象 : どのようなトレンドや周期をもち、それに従って今後どのように変化するか\n",
    "- RNNの処理概要\n",
    "  - 順伝播時、一つ前の計算結果(中間層の活性化関数後の出力値)を保管しておく\n",
    "  - 次回の順伝播の計算時に **前回の出力結果 *  w + b** の計算を行い、中間層の追加の入力情報とする\n",
    "  - 時系列モデルを扱うには、初期の状態と過去の時間t-1の状態を保持し、そこから次の時間でのtを再帰的に求める再帰構造が必要になる\n",
    "- RNNの数学的記述\n",
    "  - $u^t = W_{(in)}x^t + W z^{t-1} + b$\n",
    "  - $z^t = f( W_{(in)}x^t + W z^{t-1} + b)$\n",
    "  - $v^t = W_{(out)} z^t + c$\n",
    "  - $y^t = g( W_{(out)} z^t + c)$\n",
    "\n",
    "`調査・考察`\n",
    "\n",
    "- 時系列に対応したニューラルネットワークが RNN(Recurrent Neural Network : 再帰型ニューラルネットワーク) だが、Recurrent Neural Network を木構造に拡張した Recursive Neural Network を RNN とも略し、紛らわしいので注意。\n",
    "- 多くの場合は、RNN は Recurrent Neural Network を指している事が多いが、自然言語処理の場合は Recursive Neural Network を指す場合があるようだ。\n",
    "- 回帰型ニューラルネットワーク、循環ニューラルネットワークとも訳される\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2 : BPTT\n",
    "\n",
    "- BPTT (BackPropagation Through Time) は、 **誤差逆伝播法の一種** であり、誤差が時間をさかのぼって逆伝播させる手法\n",
    "  - RNNの誤差を求める際には時間軸で展開するとイメージしやすい\n",
    "- 誤差逆伝播法を時系列に沿う形で実施する\n",
    "- RNNにおいてのパラメータの調整方法である\n",
    "- BPTTの数学的記述\n",
    "  - その1\n",
    "    - $\\displaystyle \\frac{\\partial E}{\\partial W_{(in)}} = \\frac{\\partial E}{\\partial u^t} \\left[\\frac{\\partial u^t}{\\partial W_{(in)}} \\right]^T = \\delta^t \\left[ x^t \\right]^T$\n",
    "    - $\\displaystyle \\frac{\\partial E}{\\partial W_{(out)}} = \\frac{\\partial E}{\\partial v^t} \\left[\\frac{\\partial v^t}{\\partial W_{(out)}} \\right]^T = \\delta^{out,t} \\left[ z^t \\right]^T$\n",
    "    - $\\displaystyle \\frac{\\partial E}{\\partial W} = \\frac{\\partial E}{\\partial u^t} \\left[\\frac{\\partial u^t}{\\partial W} \\right]^T = \\delta^t \\left[ z^{t-1} \\right]^T$\n",
    "    - $\\displaystyle \\frac{\\partial E}{\\partial b} = \\frac{\\partial E}{\\partial u^t} \\frac{\\partial u^t}{\\partial b} = \\delta^t$\n",
    "    - $\\displaystyle \\frac{\\partial E}{\\partial c} = \\frac{\\partial E}{\\partial v^t} \\frac{\\partial v^t}{\\partial c} = \\delta^{out,t}$\n",
    "  - その2\n",
    "    - $\\displaystyle u^t = W_{(in)}x^t + Wz^{t-1} + b$\n",
    "    - $\\displaystyle z^t = f( W_{(in)}x^t + Wz^{t-1} + b )$\n",
    "    - $\\displaystyle v^t = W_{(out)}z^t + c$\n",
    "    - $\\displaystyle y^t = g( W_{(out)}z^t + c )$\n",
    "  - その3\n",
    "    - $\\displaystyle \\frac{\\partial E}{\\partial u^t} = \\frac{\\partial E}{\\partial v^t} \\frac{\\partial v^t}{\\partial u^t} = \\frac{\\partial E}{\\partial v^t} \\frac{\\partial \\left\\{ W_{(out)}f(u^t)+c \\right\\} }{\\partial u^t} = f'(u^t)W_{(out)}^T \\delta^{out,t} = \\delta^t$\n",
    "    - $\\displaystyle \\delta^{t-1} = \\frac{\\partial E}{\\partial u^{t-1}} = \\frac{\\partial E}{\\partial u^t} \\frac{\\partial u^t}{\\partial u^{t-1}} = \\delta^t \\left\\{ \\frac{\\partial u^t}{\\partial z^{z-t}} \\frac{\\partial z^{t-1}}{\\partial u^{t-1}} \\right\\} = \\delta^t \\left\\{ W f'(u^{t-1}) \\right\\}$\n",
    "    - $\\displaystyle \\delta^{t-z-1} = \\delta^{t-z} \\left\\{ W f'(u^{t-z-1}) \\right\\}$\n",
    "  - その４ (パラメータの更新式)\n",
    "    - $\\displaystyle W_{(in)}^{t+1} = W_{(in)}^t - \\epsilon \\frac{\\partial E}{\\partial W_{(in)}} = W_{(in)}^t - \\epsilon\\sum_{z=0}^{T_t}\\delta^{t-z} \\left[ x^{t-z} \\right]^T$\n",
    "    - $\\displaystyle W_{(out)}^{t+1} = W_{(out)}^t - \\epsilon\\frac{\\partial E}{\\partial W_{(out)}} = W_{(in)}^t - \\epsilon\\delta^{out,t} \\left[ z^t \\right]^T$\n",
    "    - $\\displaystyle W^{t+1} = W^t - \\epsilon\\frac{\\partial E}{\\partial W} = W_{(in)}^t - \\epsilon\\sum_{z=0}^{T_t}\\delta^{t-z} \\left[ z^{t-z-1} \\right]^T$\n",
    "    - $\\displaystyle b^{t+1} = b^t - \\epsilon\\frac{\\partial E}{\\partial b} = b^t - \\epsilon\\sum_{z=0}^{T_t}\\delta^{t-z}$\n",
    "    - $\\displaystyle c^{t+1} = c^t - \\epsilon\\frac{\\partial E}{\\partial c} = c^t - \\epsilon\\delta^{out,t}$\n",
    "  - BPTTの全体像\n",
    "    - $\\displaystyle \\begin{align}\n",
    "E^t &= loss \\left( y^t,d^t \\right) \\\\\n",
    "    &= loss \\left( g \\left( W_{(out)}z^t + c \\right), d^t \\right) \\\\\n",
    "    &= loss \\left( g \\left( W_{(out)} \\underline{ f(W_{(in)}x^t + Wz^{t-1}+b) } +c \\right),d^t \\right)\n",
    "\\end{align}$\n",
    "    - $\\displaystyle W_{(in)}x^t + Wz^{t-1}+b\\\\\n",
    "    W_{(in)}x^t + W f \\left( u^{t-1} \\right) + b\\\\\n",
    "    W_{(in)}x^t + W f \\left( W_{(in)}x^{t-1} + Wz^{t-2} + b \\right) + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考文献・URL\n",
    "\n",
    "- [回帰型ニューラルネットワーク - Wikipedia](https://ja.wikipedia.org/wiki/%E5%9B%9E%E5%B8%B0%E5%9E%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF)\n",
    "- [再帰型ニューラルネットワーク: RNN入門](https://qiita.com/kiminaka/items/87afd4a433dc655d8cfd)\n",
    "- [再帰型ニューラルネットワークの「基礎の基礎」を理解する　～ディープラーニング入門｜第3回](https://www.imagazine.co.jp/%E5%86%8D%E5%B8%B0%E5%9E%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%AE%E3%80%8C%E5%9F%BA%E7%A4%8E%E3%81%AE%E5%9F%BA%E7%A4%8E%E3%80%8D/)\n",
    "- [ニューラルネットワークで時系列データの予測を行う](https://qiita.com/icoxfog417/items/2791ee878deee0d0fd9c)\n",
    "- [Backpropagation through time - wikipedia](https://en.wikipedia.org/wiki/Backpropagation_through_time)\n",
    "- [第4回　Backpropagation Through Time（BPTT）](https://book.mynavi.jp/manatee/detail/id=76172)\n",
    "- [Backpropagation Through Time - The First Cry of Atom](https://www.lewuathe.com/backpropagation-through-time.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認テスト\n",
    "\n",
    "- Q : サイズ 5x5 の入力画像を、サイズ 3x3 のフィルタで畳み込んだ時の出力画像のサイズを答えよ。なおスライドは2、パディングは１とする。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- 出力画像の $H = \\frac{H+2padding-filterH}{stride}+1 = \\frac{5 + 2 * 1 - 3}{2} + 1 = 3$\n",
    "- 出力画像の $W = \\frac{H+2padding-filterW}{stride}+1 = \\frac{5 + 2 * 1 - 3}{2} + 1 = 3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : RNNのネットワークには大きく分けて３つの重みがある。  \n",
    "１つは入力から現在の中間層を定義する際にかけられる重み、１つは中間層から出力を定義する際にかけられる重みである。  \n",
    "残り１つの重みについて説明せよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- RNNは前回の順伝播時の中間層出力を現在の中間層の入力とするが、そこには W,b を他と同様に重み・バイアスとして演算する。残り１つの重みはこれである。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : 連鎖率の原理を使い、 $\\frac{\\Delta z}{\\Delta x}$ を求めよ。\n",
    "$$z = t^2$$\n",
    "$$t = x + y$$\n",
    "\n",
    "`考察(再掲)`\n",
    "\n",
    "$$ \\frac{\\Delta z}{\\Delta x} = \\frac{\\Delta z}{\\Delta t} \\cdot \\frac{\\Delta t}{\\Delta x} = 2t = 2(x + y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : 下図の $y_1$ を $x \\cdot s_0 \\cdot s_1 \\cdot w_{in} \\cdot w \\cdot w_{out}$ を用いて数式で表わせ。  \n",
    "※バイアスは任意の文字で定義せよ。  \n",
    "※また中間層の出力にシグモイド関数 $g(x)$ を作用させよ。\n",
    "\n",
    "<img src=\"./images/day3/section1_q4.png\" width=\"750px\" />\n",
    "\n",
    "`考察`\n",
    "\n",
    "- $z_1 = sigmoid(S_0 W + x_1 W_{(in)} + b)$\n",
    "- $y_1 = sigmoid(Z_1 W_{(out)} + c)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習の結果と考察\n",
    "\n",
    "`実施結果`\n",
    "\n",
    "- [演習 3_1_simple_RNN](./03_exercise/lesson_3/3_1_simple_RNN.ipynb)\n",
    "    - 変更箇所は【レポート提出者変更】とコメントで囲って記載\n",
    "\n",
    "`考察`\n",
    "\n",
    "- 活性化関数がシグモイドの場合、隠れ層（中間層）のノード数を64まで増やす範囲なら精度が向上することを観測できた。\n",
    "- パラメータの初期値（正規分布の乱数生成パラメータ）について、1.25程度まで上げる範囲なら精度が向上することを観測できた。それ以上の場合は不安定となり収束が遅くなった。\n",
    "- 学習率は0.15程度まで上げた時点で、精度が向上していることを観測できた。それ以上の場合は不安定となり収束が遅くなった。\n",
    "- 重みの初期化方法は、Heを用いると学習速度が早まった。Xavierは正答率のブレは少なくなるが学習速度は早まらなかった。\n",
    "- 活性化関数をシグモイド関数 から ReLU 関数に変更後、勾配爆発が発生。正答率が不安定かつ学習されなくなり、全く精度が向上しなかった。\n",
    "- 活性化関数を ReLU から tanh に変更すると、正答率が安定し学習速度が圧倒的に早まった。\n",
    "- 活性化関数が tanh の場合は精度が向上したため、hidden_layer_sizeを32,64と組み合わせてみた。勾配爆発が発生し、逆に安定になくなってしまった。\n",
    "- どれか一つのパラメータ・活性化関数を選択・調整すればよいわけではなく、精度が良くなる想定が悪い組み合わせとなってしまう場合があり、簡単ではない。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 変更対象 | 誤差グラフ | - | - | - | - | 備考 |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| hidden_layer_size | <img src=\"./images/day3/3_1_hidden_layer_size_8.png\" width=\"750px\" /><br>8 | <img src=\"./images/day3/3_1_hidden_layer_size_16.png\" width=\"750px\" /><br>**16 (計測基準)** | <img src=\"./images/day3/3_1_hidden_layer_size_32.png\" width=\"750px\" /><br>32 | <img src=\"./images/day3/3_1_hidden_layer_size_64.png\" width=\"750px\" /><br>64 | <img src=\"./images/day3/3_1_hidden_layer_size_128.png\" width=\"750px\" /><br>128 | - |\n",
    "| weight_init_std | <img src=\"./images/day3/3_1_wait_init_std_0.50.png\" width=\"750px\" /><br>0.50 | <img src=\"./images/day3/3_1_wait_init_std_0.75.png\" width=\"750px\" /><br>0.75 | <img src=\"./images/day3/3_1_wait_init_std_1.00.png\" width=\"750px\" /><br>**1.00 (計測基準)** | <img src=\"./images/day3/3_1_wait_init_std_1.25.png\" width=\"750px\" /><br>1.25 | <img src=\"./images/day3/3_1_wait_init_std_1.50.png\" width=\"750px\" /><br>1.50 | - |\n",
    "| learning_rate | <img src=\"./images/day3/3_1_learning_rate_0.01.png\" width=\"750px\" /><br>0.01 | <img src=\"./images/day3/3_1_learning_rate_0.05.png\" width=\"750px\" /><br>0.05 | <img src=\"./images/day3/3_1_learning_rate_0.10.png\" width=\"750px\" /><br>**0.10 (計測基準)** | <img src=\"./images/day3/3_1_learning_rate_0.15.png\" width=\"750px\" /><br>0.15 | <img src=\"./images/day3/3_1_learning_rate_0.20.png\" width=\"750px\" /><br>0.20 | - |\n",
    "| 重みの初期化方法 | <img src=\"./images/day3/3_1_default.png\" width=\"750px\" /><br>**None (計測基準)** | <img src=\"./images/day3/3_1_initial_xavier.png\" width=\"750px\" /><br>Xavier | <img src=\"./images/day3/3_1_initial_he.png\" width=\"750px\" /><br>He | - | - | - |\n",
    "| 中間層の活性化関数 | <img src=\"./images/day3/3_1_initial_he.png\" width=\"750px\" /><br>**sigmoid (計測基準)** | <img src=\"./images/day3/3_1_activation_relu.png\" width=\"750px\" /><br>ReLU <span style=\"color: red\">**(勾配爆発)**</span> | <img src=\"./images/day3/3_1_activation_tanh.png\" width=\"750px\" /><br>tanh | <img src=\"./images/day3/3_1_activation_tanh_hidden_layer_size_32.png\" width=\"750px\" /><br>tanh(hidden_layer_size:32) <span style=\"color: red\">**(勾配爆発)**</span> | <img src=\"./images/day3/3_1_activation_tanh_hidden_layer_size_64.png\" width=\"750px\" /><br>tanh(hidden_layer_size:64) <span style=\"color: red\">**(勾配爆発)**</span> | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c8XKJrfWVZvl"
   },
   "source": [
    "## Section2 : LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1 : CEC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 : 入力ゲートと出力ゲート\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3 : 忘却ゲート\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4 : 覗き穴結合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考文献・URL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認テスト\n",
    "\n",
    "- Q : シグモイド関数を微分したとき、入力値が０の時に最大値を取る。その値として正しいものを選択肢から選べ。\n",
    "  - (1) 0.15\n",
    "  - (2) 0.25\n",
    "  - (3) 0.35\n",
    "  - (4) 0.45\n",
    "\n",
    "`考察（再掲）`\n",
    "\n",
    "  - シグモイド関数 -> $sigmoid(x)$  \n",
    "  - シグモイド関数の導関数 -> $(1 - sigmoid(x)) * sigmoid(x)$  \n",
    "  - $sigmoid(0) = 0.5$ なので導関数の値は $(1 - 0.5) * 0.5 = 0.25$\n",
    "    - よって **(2) 0.25**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjGuhvuAVhMI"
   },
   "source": [
    "## Section3 : GRU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section4 : 双方向RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNでの自然言語処理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section5 : Seq2Seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1 : Encoder RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2 : Decoder RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3 : HRED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-4 : VHRED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-5 : VAE\n",
    "\n",
    "- オートエンコーダー\n",
    "\n",
    "- VAE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section6 : Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section7 : Attention Mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 深層学習 後半2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1 : Tensorflow の実装演習\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2 : 強化学習\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "04_深層学習_後半.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
