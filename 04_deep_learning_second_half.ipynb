{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4fhror4LEgr"
   },
   "source": [
    "iStudy ACADEMY 現場で潰しが効くディープラーニング講座 視聴課題レポート 深層学習 後半1,2（講義動画と実装演習）\n",
    "<div style=\"text-align: right;\">【レポート作成者】S.Honda</div>\n",
    "\n",
    "# 深層学習 後半1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 再帰型ニューラルネットワークについて\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1 : 再帰型ニューラルネットワークの概念\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erWaiWY6VPQ9"
   },
   "source": [
    "### 1-1 : RNN全体像\n",
    "\n",
    "- RNNとは\n",
    "  - RNN (Recurrent Neural Network : 再帰型ニューラルネットワーク) は、ニューラルネットワークを拡張し時系列データを扱えるようにしたもの\n",
    "- 時系列データとは\n",
    "  - 時間的順序を追って一定間隔ごとに観察され、しかも相互に統計的依存関係が認められるようなデータの系列\n",
    "    - ある時刻の値は、以前の時刻の変化の延長上\n",
    "    - ある時間の経過とともに値が変化していくようなデータ\n",
    "  - データの例\n",
    "    - 音声データ, テキストデータ, 株価データなど\n",
    "    - 店舗の日次売上データ, ホームページのアクセス数履歴, 工場設備のセンサデータなどなど\n",
    "  - 関心の対象 : どのようなトレンドや周期をもち、それに従って今後どのように変化するか\n",
    "- RNNの処理概要\n",
    "  - 順伝播時、一つ前の計算結果(中間層の活性化関数後の出力値)を保管しておく\n",
    "  - 次回の順伝播の計算時に **前回の出力結果 *  w + b** の計算を行い、中間層の追加の入力情報とする\n",
    "- RNNの数学的記述\n",
    "  - $u^t = W_{(in)}x^t + W z^{t-1} + b$\n",
    "  - $z^t = f( W_{(in)}x^t + W z^{t-1} + b)$\n",
    "  - $v^t = W_{(out)} z^t + c$\n",
    "  - $y^t = g( W_{(out)} z^t + c$\n",
    "\n",
    "`調査・考察`\n",
    "\n",
    "- 時系列に対応したニューラルネットワークが RNN(Recurrent Neural Network : 再帰型ニューラルネットワーク) だが、Recurrent Neural Network を木構造に拡張した Recursive Neural Network を RNN とも略し、紛らわしいので注意。\n",
    "- 多くの場合は、RNN は Recurrent Neural Network を指している事が多いが、自然言語処理の場合は Recursive Neural Network を指す場合があるようだ。\n",
    "- 回帰型ニューラルネットワーク、循環ニューラルネットワークとも訳される\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2 : BPTT\n",
    "\n",
    "- BPTT (BackPropagation Through Time) は、 **誤差逆伝播法の一種** であり、誤差が時間をさかのぼって逆伝播させる手法\n",
    "  - RNNの誤差を求める際には時間軸で展開するとイメージしやすい\n",
    "- 誤差逆伝播法を時系列に沿う形で実施する\n",
    "- RNNにおいてのパラメータの調整方法である\n",
    "- BPTTの数学的記述\n",
    "  - その1\n",
    "    - $\\displaystyle \\frac{\\partial E}{\\partial W_{(in)}} = \\frac{\\partial E}{\\partial u^t} \\left[\\frac{\\partial u^t}{\\partial W_{(in)}} \\right]^T = \\delta^t \\left[ x^t \\right]^T$\n",
    "    - $\\displaystyle \\frac{\\partial E}{\\partial W_{(out)}} = \\frac{\\partial E}{\\partial v^t} \\left[\\frac{\\partial v^t}{\\partial W_{(out)}} \\right]^T = \\delta^{out,t} \\left[ z^t \\right]^T$\n",
    "    - $\\displaystyle \\frac{\\partial E}{\\partial W} = \\frac{\\partial E}{\\partial u^t} \\left[\\frac{\\partial u^t}{\\partial W} \\right]^T = \\delta^t \\left[ z^{t-1} \\right]^T$\n",
    "    - $\\displaystyle \\frac{\\partial E}{\\partial b} = \\frac{\\partial E}{\\partial u^t} \\frac{\\partial u^t}{\\partial b} = \\delta^t$\n",
    "    - $\\displaystyle \\frac{\\partial E}{\\partial c} = \\frac{\\partial E}{\\partial v^t} \\frac{\\partial v^t}{\\partial c} = \\delta^{out,t}$\n",
    "  - その2\n",
    "    - $\\displaystyle u^t = W_{(in)}x^t + Wz^{t-1} + b$\n",
    "    - $\\displaystyle z^t = f( W_{(in)}x^t + Wz^{t-1} + b )$\n",
    "    - $\\displaystyle v^t = W_{(out)}z^t + c$\n",
    "    - $\\displaystyle y^t = g( W_{(out)}z^t + c )$\n",
    "  - その3\n",
    "    - $\\displaystyle \\frac{\\partial E}{\\partial u^t} = \\frac{\\partial E}{\\partial v^t} \\frac{\\partial v^t}{\\partial u^t} = \\frac{\\partial E}{\\partial v^t} \\frac{\\partial \\left\\{ W_{(out)}f(u^t)+c \\right\\} }{\\partial u^t} = f'(u^t)W_{(out)}^T \\delta^{out,t} = \\delta^t$\n",
    "    - $\\displaystyle \\delta^{t-1} = \\frac{\\partial E}{\\partial u^{t-1}} = \\frac{\\partial E}{\\partial u^t} \\frac{\\partial u^t}{\\partial u^{t-1}} = \\delta^t \\left\\{ \\frac{\\partial u^t}{\\partial z^{z-t}} \\frac{\\partial z^{t-1}}{\\partial u^{t-1}} \\right\\} = \\delta^t \\left\\{ W f'(u^{t-1}) \\right\\}$\n",
    "    - $\\displaystyle \\delta^{t-z-1} = \\delta^{t-z} \\left\\{ W f'(u^{t-z-1}) \\right\\}$\n",
    "  - その４ (パラメータの更新式)\n",
    "    - $\\displaystyle W_{(in)}^{t+1} = W_{(in)}^t - \\epsilon \\frac{\\partial E}{\\partial W_{(in)}} = W_{(in)}^t - \\epsilon\\sum_{z=0}^{T_t}\\delta^{t-z} \\left[ x^{t-z} \\right]^T$\n",
    "    - $\\displaystyle W_{(out)}^{t+1} = W_{(out)}^t - \\epsilon\\frac{\\partial E}{\\partial W_{(out)}} = W_{(in)}^t - \\epsilon\\delta^{out,t} \\left[ z^t \\right]^T$\n",
    "    - $\\displaystyle W^{t+1} = W^t - \\epsilon\\frac{\\partial E}{\\partial W} = W_{(in)}^t - \\epsilon\\sum_{z=0}^{T_t}\\delta^{t-z} \\left[ z^{t-z-1} \\right]^T$\n",
    "    - $\\displaystyle b^{t+1} = b^t - \\epsilon\\frac{\\partial E}{\\partial b} = b^t - \\epsilon\\sum_{z=0}^{T_t}\\delta^{t-z}$\n",
    "    - $\\displaystyle c^{t+1} = c^t - \\epsilon\\frac{\\partial E}{\\partial c} = c^t - \\epsilon\\delta^{out,t}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考文献・URL\n",
    "\n",
    "- [回帰型ニューラルネットワーク - Wikipedia](https://ja.wikipedia.org/wiki/%E5%9B%9E%E5%B8%B0%E5%9E%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF)\n",
    "- [再帰型ニューラルネットワーク: RNN入門](https://qiita.com/kiminaka/items/87afd4a433dc655d8cfd)\n",
    "- [再帰型ニューラルネットワークの「基礎の基礎」を理解する　～ディープラーニング入門｜第3回](https://www.imagazine.co.jp/%E5%86%8D%E5%B8%B0%E5%9E%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%AE%E3%80%8C%E5%9F%BA%E7%A4%8E%E3%81%AE%E5%9F%BA%E7%A4%8E%E3%80%8D/)\n",
    "- [ニューラルネットワークで時系列データの予測を行う](https://qiita.com/icoxfog417/items/2791ee878deee0d0fd9c)\n",
    "- [Backpropagation through time - wikipedia](https://en.wikipedia.org/wiki/Backpropagation_through_time)\n",
    "- [第4回　Backpropagation Through Time（BPTT）](https://book.mynavi.jp/manatee/detail/id=76172)\n",
    "- [Backpropagation Through Time - The First Cry of Atom](https://www.lewuathe.com/backpropagation-through-time.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認テスト\n",
    "\n",
    "- Q : サイズ 5x5 の入力画像を、サイズ 3x3 のフィルタで畳み込んだ時の出力画像のサイズを答えよ。なおスライドは2、パディングは１とする。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- 出力画像の $H = \\frac{H+2padding-filterH}{stride}+1 = \\frac{5 + 2 * 1 - 3}{2} + 1 = 3$\n",
    "- 出力画像の $W = \\frac{H+2padding-filterW}{stride}+1 = \\frac{5 + 2 * 1 - 3}{2} + 1 = 3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : RNNのネットワークには大きく分けて３つの重みがある。  \n",
    "１つは入力から現在の中間層を定義する際にかけられる重み、１つは中間層から出力を定義する際にかけられる重みである。  \n",
    "残り１つの重みについて説明せよ。\n",
    "\n",
    "`考察`\n",
    "\n",
    "- RNNは前回の順伝播時の中間層出力を現在の中間層の入力とするが、そこには W,b を他と同様に重み・バイアスとして演算する。残り１つの重みはこれである。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : 連鎖率の原理を使い、 $\\frac{\\Delta z}{\\Delta x}$ を求めよ。\n",
    "$$z = t^2$$\n",
    "$$t = x + y$$\n",
    "\n",
    "`考察(再掲)`\n",
    "\n",
    "$$ \\frac{\\Delta z}{\\Delta x} = \\frac{\\Delta z}{\\Delta t} \\cdot \\frac{\\Delta t}{\\Delta x} = 2t = 2(x + y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q : 下図の $y_1$ を $x \\cdot s_0 \\cdot s_1 \\cdot w_{in} \\cdot w \\cdot w_{out}$ を用いて数式で表わせ。  \n",
    "※バイアスは任意の文字で定義せよ。  \n",
    "※また中間層の出力にシグモイド関数 $g(x)$ を作用させよ。\n",
    "\n",
    "<img src=\"./images/day3/section1_q4.png\" width=\"750px\" />\n",
    "\n",
    "`考察`\n",
    "\n",
    "```WRITE DOC HERE!```  \n",
    "```WRITE DOC HERE!```  \n",
    "```WRITE DOC HERE!```  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習の結果と考察\n",
    "\n",
    "`実施結果`\n",
    "\n",
    "- [演習 3_1_simple_RNN](./04_exercise/lesson_3/3_1_simple_RNN.ipynb)\n",
    "    - 変更箇所は【レポート提出者変更】とコメントで囲って記載\n",
    "\n",
    "`考察`\n",
    "\n",
    "```WRITE DOC HERE!```  \n",
    "```WRITE DOC HERE!```  \n",
    "```WRITE DOC HERE!```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c8XKJrfWVZvl"
   },
   "source": [
    "## Section2 : LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1 : CEC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 : 入力ゲートと出力ゲート\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3 : 忘却ゲート\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4 : 覗き穴結合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考文献・URL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjGuhvuAVhMI"
   },
   "source": [
    "## Section3 : GRU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section4 : 双方向RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNでの自然言語処理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section5 : Seq2Seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1 : Encoder RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2 : Decoder RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3 : HRED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-4 : VHRED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-5 : VAE\n",
    "\n",
    "- オートエンコーダー\n",
    "\n",
    "- VAE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section6 : Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section7 : Attention Mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 深層学習 後半2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1 : Tensorflow の実装演習\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2 : 強化学習\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "04_深層学習_後半.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
